<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook (Posts about learning)</title><link>https://laurentperrinet.github.io/sciblog/</link><description></description><atom:link href="https://laurentperrinet.github.io/sciblog/categories/learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Tue, 24 Nov 2020 14:41:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Sparse coding of large images</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-12-13-sparse-coding-using-fista.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post we would try to show how one could infer the sparse representation of an image knowing an appropriate generative model for its synthesis. We will start by a linear inversion (pseudo-inverse deconvolution), and then move to a gradient descent algorithm. Finally, we will implement a convolutional version of the iterative shrinkage thresholding algorithm (ISTA) and its fast version, the FISTA.&lt;/p&gt;
&lt;p&gt;For computational efficiency, all convolutions will be implemented by a &lt;a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform"&gt;Fast Fourier Tansform&lt;/a&gt;, so that a standard convolution will be mathematically exactly similar. We will benchmark this on a realistic image size of $512 \times 512$ giving some timing results on a standard laptop.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-12-13-sparse-coding-using-fista.html"&gt;Read more…&lt;/a&gt; (31 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>learning</category><category>Matching Pursuit</category><category>neural</category><category>sparse</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-12-13-sparse-coding-using-fista.html</guid><pubDate>Thu, 13 Dec 2018 09:53:03 GMT</pubDate></item><item><title>extending datasets in pyTorch</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://pytorch.org"&gt;PyTorch&lt;/a&gt; is a great library for machine learning. You can in a few lines of codes retrieve a dataset, define your model, add a cost function and then train your model. It's quite magic to copy and paste code from the internet and get the &lt;a href="https://github.com/pytorch/examples/tree/master/mnist"&gt;LeNet network&lt;/a&gt; working in a few seconds to achieve more than 98% accuracy.&lt;/p&gt;
&lt;p&gt;However, it can be tedious sometimes to extend existing objects and here, I will manipulate some ways to define the right dataset for your application. In particular I will modify the call to a standard dataset (&lt;a href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist"&gt;MNIST&lt;/a&gt;) to place the characters at random places in a large image.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>deep-learning</category><category>ipython</category><category>learning</category><category>machine-learning</category><category>neural</category><category>numpy</category><category>open-science</category><category>python</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html</guid><pubDate>Fri, 07 Sep 2018 10:27:10 GMT</pubDate></item><item><title>MEUL with a non-parametric homeostasis</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="https://laurentperrinet.github.io/publication/perrinet-10-shl"&gt;https://laurentperrinet.github.io/publication/perrinet-10-shl&lt;/a&gt; ). Compared to other posts, such as this &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;previous post&lt;/a&gt;, we improve the code to not depend on any parameter (namely the &lt;code&gt;C&lt;/code&gt;parameter of the rescaling function). For that, we will use a non-parametric approach based on the use of cumulative histograms.&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/victor-boutin"&gt;Victor Boutin&lt;/a&gt; and &lt;a href="https://laurentperrinet.github.io/author/angelo-franciosini/"&gt;Angelo Francisioni&lt;/a&gt;. See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html"&gt;Read more…&lt;/a&gt; (47 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html</guid><pubDate>Tue, 07 Nov 2017 14:13:04 GMT</pubDate></item><item><title>Improving calls to the LogGabor library</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-10-06-improving-calls-to-the-loggabor-library.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;To code image as edges, for instance in the &lt;code&gt;SparseEdges&lt;/code&gt; &lt;a href="https://github.com/bicv/SparseEdges"&gt;sparse coding scheme&lt;/a&gt;, we use a model of edges in images. A good model for these edges are &lt;a href="https://en.wikipedia.org/wiki/Log_Gabor_filter#Bi-dimensional_Log-Gabor_filter"&gt;bidimensional Log Gabor filter&lt;/a&gt;. This is implemented for instance in the &lt;code&gt;LogGabor&lt;/code&gt; library. The library was designed to be precise, but not particularly for efficiency. In order to improve its speed, we demonstrate here the use of a cache to avoid redundant computations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-10-06-improving-calls-to-the-loggabor-library.html"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>ipython</category><category>learning</category><category>LogGabor</category><category>Matching Pursuit</category><category>numpy</category><category>python</category><category>SLIP</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-10-06-improving-calls-to-the-loggabor-library.html</guid><pubDate>Fri, 06 Oct 2017 09:04:04 GMT</pubDate></item><item><title>The fastest 2D convolution in the world</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Convolutions are essential components of any neural networks, image processing, computer vision ... but these are also a bottleneck in terms of computations... I will here benchmark different solutions using &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; or &lt;code&gt;pytorch&lt;/code&gt;. This is work-in-progress, so that any suggestion is welcome, for instance on &lt;a href="https://dsp.stackexchange.com/questions/43953/looking-for-fastest-2d-convolution-in-python-on-a-cpu"&gt;StackExchange&lt;/a&gt; or in the comments below this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>behavior</category><category>ipython</category><category>learning</category><category>neural</category><category>numpy</category><category>open-science</category><category>outreach</category><category>python</category><category>tensorflow</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html</guid><pubDate>Wed, 20 Sep 2017 09:13:10 GMT</pubDate></item><item><title>Le jeu de l'urne</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-06-15-le-jeu-de-lurne.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;figure&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2015-12-08_cours_neurocomp/figures/questions.png"&gt;&lt;/figure&gt; &lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Lors de la visite au laboratoire d'une brillante élève de seconde (salut Lena!), nous avons inventé ensemble un jeu: &lt;em&gt;le jeu de l'urne&lt;/em&gt;. Le principe est simple: il faut deviner la couleur de la balle qu'on tire d'une urne contenant autant de balles rouges que noires - et ceci le plus tôt possible. Plus précisément, les règles sont:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On a un ensemble de balles, la motié sont rouges, l'autre moitié noires (c'est donc un nombre pair de balles qu'on appelera $N$, disons $N=8$). &lt;/li&gt;
&lt;li&gt;Elles sont dans une urne opaque et donc on ne peut pas les voir à moins de les tirer une par une (sans remise dans l'urne). On peut tirer autant de balles qu'on veut pour les observer.&lt;/li&gt;
&lt;li&gt;Le but est de deviner la balle qu'on va tirer. Si on gagne (on a bien prédit la couleur), alors on gagne autant de points que le nombre de balles qui étaient dans l'urne au moment de la décision. Sinon on perd autant de points que l'on en aurait gagné! &lt;/li&gt;
&lt;li&gt;à long terme, la stratégie du jeu est de décider le meilleur moment où on est prêt à deviner la couleur de la balle qu'on va prendre et ainsi de gagner le plus de points possibles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nous avons d'abord créé ce jeu grâce au language de programmation &lt;a href="//scratch.mit.edu"&gt;Scratch&lt;/a&gt; sur &lt;a href="https://scratch.mit.edu/projects/165806365/"&gt;https://scratch.mit.edu/projects/165806365/&lt;/a&gt;:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;iframe allowtransparency="true" width="485" height="402" src="//scratch.mit.edu/projects/embed/165806365/?autostart=false" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Ici, nous allons essayer de l'analyser plus finement.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-06-15-le-jeu-de-lurne.html"&gt;Read more…&lt;/a&gt; (37 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>behavior</category><category>learning</category><category>open-science</category><category>outreach</category><category>psychophysics</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-06-15-le-jeu-de-lurne.html</guid><pubDate>Thu, 15 Jun 2017 14:42:44 GMT</pubDate></item><item><title>testing COMPs-fastPcum_scripted</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="https://laurentperrinet.github.io/publication/perrinet-10-shl"&gt;https://laurentperrinet.github.io/publication/perrinet-10-shl&lt;/a&gt; ). Compared to the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;previous post&lt;/a&gt;, we integrated the faster code to &lt;a href="https://github.com/bicv/SHL_scripts"&gt;https://github.com/bicv/SHL_scripts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/victor-boutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html</guid><pubDate>Tue, 23 May 2017 14:13:15 GMT</pubDate></item><item><title>testing COMPs-fastPcum</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="https://laurentperrinet.github.io/publication/perrinet-10-shl"&gt;https://laurentperrinet.github.io/publication/perrinet-10-shl&lt;/a&gt; ). Compared to the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html"&gt;previous post&lt;/a&gt;, we optimize the code to be faster.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/victor-boutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;Read more…&lt;/a&gt; (52 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html</guid><pubDate>Tue, 23 May 2017 14:13:04 GMT</pubDate></item><item><title>testing COMPs-Pcum</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="https://laurentperrinet.github.io/publication/perrinet-10-shl"&gt;https://laurentperrinet.github.io/publication/perrinet-10-shl&lt;/a&gt; ). In particular, we will show how one can build the non-linear functions based on the activity of each filter and which implement homeostasis.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/victor-boutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html"&gt;Read more…&lt;/a&gt; (21 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html</guid><pubDate>Tue, 23 May 2017 13:58:37 GMT</pubDate></item><item><title>Extending Olshausens classical SparseNet</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-27-extending-olshausens-classical-sparsenet.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In a &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-15-reproducing-olshausens-classical-sparsenet-part-2.html"&gt;previous notebook&lt;/a&gt;, we tried to reproduce the learning strategy specified in the framework of the &lt;a href="http://redwood.berkeley.edu/bruno/sparsenet/"&gt;SparseNet algorithm from Bruno Olshausen&lt;/a&gt;. It allows to efficiently code natural image patches by constraining the code to be sparse. In particular, we saw that in order to optimize competition, it is important to control cooperation and we implemented a heuristic to just do this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this notebook, we provide an extension to the SparseNet algorithm. We will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="https://laurentperrinet.github.io/publication/perrinet-10-shl"&gt;https://laurentperrinet.github.io/publication/perrinet-10-shl&lt;/a&gt; ):&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nc"&gt;@article&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;Perrinet10shl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Title&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Role of homeostasis in learning sparse representations}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Author&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Perrinet, Laurent U.}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Journal&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Neural Computation}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Year&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{2010}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Doi&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{10.1162/neco.2010.05-08-795}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Keywords&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{Neural population coding, Unsupervised learning, Statistics of natural images, Simple cell receptive fields, Sparse Hebbian Learning, Adaptive Matching Pursuit, Cooperative Homeostasis, Competition-Optimized Matching Pursuit}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Month&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{July}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Number&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{7}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Url&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{https://laurentperrinet.github.io/publication/perrinet-10-shl}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="na"&gt;Volume&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;{22}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/victor-boutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-27-extending-olshausens-classical-sparsenet.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>learning</category><category>open-science</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-27-extending-olshausens-classical-sparsenet.html</guid><pubDate>Mon, 27 Mar 2017 08:49:34 GMT</pubDate></item></channel></rss>