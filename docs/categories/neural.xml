<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook (Posts about neural)</title><link>https://laurentperrinet.github.io/sciblog/</link><description></description><atom:link href="https://laurentperrinet.github.io/sciblog/categories/neural.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Thu, 31 Jan 2019 11:42:20 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Statistics of the natural input to a ring model</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;figure&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2018-11-05_Ring_input.png"&gt;&lt;/figure&gt; &lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;What does the input to a population of neurons in the primary visual cortex look like? In this post, we will try to have a feeling of the structure and statistics of the natural input to such a "ring" model.&lt;/p&gt;
&lt;p&gt;This notebook explores this question using a retina-like temporal filtering and oriented Gabor-like filters. It produces this polar plot of the instantaneous energy in the different orientations for a natural movie :&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;video controls autoplay loop src="https://laurentperrinet.github.io/sciblog/files/2018-11-05_Ring_input.mp4" width="61.8%/"&gt; &lt;/video&gt;&lt;/center&gt;
&lt;br&gt;

&lt;p&gt;One observes different striking features in the structure of this input to populations of V1 neurons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input is sparse: often, a few orientations dominate - the shape of these components (bandwidth) seem to be similar,&lt;/li&gt;
&lt;li&gt;there are many "switches": at some moments, the representations flips to another. This is due to cuts in the movie (changes from one scene to the other for instance). In a more realistic setting where we would add eye movements, these switches should also happen during saccades (but is there any knowledge of the occurence of the switch by the visual system?),&lt;/li&gt;
&lt;li&gt;between switches, there is some coherence in amplitude (a component will slowly change its energy) but also in time (a component is more likely to have a ghradually changing oriantation, for instance when the scene rotates).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structure is specific to the structure of natural images and to the way they transform (translations, rotations, zooms due to the motion and deformation of visual objects). This is certainly incorporated as a "prior" information in the structure of the visual cortex. As to know how and where this is implemented is an open scientific question.&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="http://invibe.net/LaurentPerrinet/HugoLadret"&gt;Hugo Ladret&lt;/a&gt;.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html"&gt;Read more…&lt;/a&gt; (597 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>LogGabor</category><category>machine-learning</category><category>motion</category><category>motionclouds</category><category>neural</category><category>numpy</category><category>orientation</category><category>psychophysics</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html</guid><pubDate>Mon, 05 Nov 2018 14:41:07 GMT</pubDate></item><item><title>extending datasets in pyTorch</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://pytorch.org"&gt;PyTorch&lt;/a&gt; is a great library for machine learning. You can in a few lines of codes retrieve a dataset, define your model, add a cost function and then train your model. It's quite magic to copy and paste code from the internet and get the &lt;a href="https://github.com/pytorch/examples/tree/master/mnist"&gt;LeNet network&lt;/a&gt; working in a few seconds to achieve more than 98% accuracy.&lt;/p&gt;
&lt;p&gt;However, it can be tedious sometimes to extend existing objects and here, I will manipulate some ways to define the right dataset for your application. In particular I will modify the call to a standard dataset (&lt;a href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist"&gt;MNIST&lt;/a&gt;) to place the characters at random places in a large image.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>deep-learning</category><category>ipython</category><category>learning</category><category>machine-learning</category><category>neural</category><category>numpy</category><category>open-science</category><category>python</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-09-07-extending-datasets-in-pytorch.html</guid><pubDate>Fri, 07 Sep 2018 10:27:10 GMT</pubDate></item><item><title>predictive-coding of variable motion</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-12-21-predictive-coding-of-variable-motion.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In some recent modeling work:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Laurent Perrinet, Guillaume S. Masson. Motion-based prediction is sufficient to solve the aperture problem. Neural Computation, 24(10):2726--50, 2012 &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet12pred"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet12pred&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;we study the role of transport in modifying our perception of motion. Here, we test what happens when we change the amount of noise in the stimulus.&lt;/p&gt;
&lt;p&gt;In this script the predictive coding is done using the &lt;code&gt;MotionParticles&lt;/code&gt; package and for a &lt;a href="https://laurentperrinet.github.io/sciblog/posts/motion%20texture"&gt;http://motionclouds.invibe.net/&lt;/a&gt; within a disk aperture.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-12-21-predictive-coding-of-variable-motion.html"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>motion</category><category>neural</category><category>open-science</category><category>predictive coding</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-12-21-predictive-coding-of-variable-motion.html</guid><pubDate>Thu, 21 Dec 2017 05:58:24 GMT</pubDate></item><item><title>MEUL with a non-parametric homeostasis</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl&lt;/a&gt; ). Compared to other posts, such as this &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;previous post&lt;/a&gt;, we improve the code to not depend on any parameter (namely the &lt;code&gt;C&lt;/code&gt;parameter of the rescaling function). For that, we will use a non-parametric approach based on the use of cumulative histograms.&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="http://invibe.net/LaurentPerrinet/VictorBoutin"&gt;Victor Boutin&lt;/a&gt; and &lt;a href="http://invibe.net/LaurentPerrinet/AngeloFrancisioni"&gt;Angelo Francisioni&lt;/a&gt;. See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html"&gt;Read more…&lt;/a&gt; (47 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-11-07-meul-with-a-non-parametric-homeostasis.html</guid><pubDate>Tue, 07 Nov 2017 14:13:04 GMT</pubDate></item><item><title>The fastest 2D convolution in the world</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Convolutions are essential components of any neural networks, image processing, computer vision ... but these are also a bottleneck in terms of computations... I will here benchmark different solutions using &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; or &lt;code&gt;pytorch&lt;/code&gt;. This is work-in-progress, so that any suggestion is welcome, for instance on &lt;a href="https://dsp.stackexchange.com/questions/43953/looking-for-fastest-2d-convolution-in-python-on-a-cpu"&gt;StackExchange&lt;/a&gt; or in the comments below this post.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html"&gt;Read more…&lt;/a&gt; (13 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>behavior</category><category>ipython</category><category>learning</category><category>neural</category><category>numpy</category><category>open-science</category><category>outreach</category><category>python</category><category>tensorflow</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-09-20-the-fastest-2d-convolution-in-the-world.html</guid><pubDate>Wed, 20 Sep 2017 09:13:10 GMT</pubDate></item><item><title>testing COMPs-fastPcum_scripted</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl&lt;/a&gt; ). Compared to the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;previous post&lt;/a&gt;, we integrated the faster code to &lt;a href="https://github.com/bicv/SHL_scripts"&gt;https://github.com/bicv/SHL_scripts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="http://invibe.net/LaurentPerrinet/VictorBoutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum_scripted.html</guid><pubDate>Tue, 23 May 2017 14:13:15 GMT</pubDate></item><item><title>testing COMPs-fastPcum</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl&lt;/a&gt; ). Compared to the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html"&gt;previous post&lt;/a&gt;, we optimize the code to be faster.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="http://invibe.net/LaurentPerrinet/VictorBoutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html"&gt;Read more…&lt;/a&gt; (52 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-fastpcum.html</guid><pubDate>Tue, 23 May 2017 14:13:04 GMT</pubDate></item><item><title>testing COMPs-Pcum</title><link>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this notebook, we will study how homeostasis (cooperation) may be an essential ingredient to this algorithm working on a winner-take-all basis (competition). This extension has been published as Perrinet, Neural Computation (2010) (see  &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet10shl&lt;/a&gt; ). In particular, we will show how one can build the non-linear functions based on the activity of each filter and which implement homeostasis.&lt;/p&gt;
&lt;p&gt;See also the other posts on &lt;a href="https://laurentperrinet.github.io/sciblog/categories/learning.html"&gt;unsupervised learning&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="http://invibe.net/LaurentPerrinet/VictorBoutin"&gt;Victor Boutin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html"&gt;Read more…&lt;/a&gt; (21 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>bicv</category><category>learning</category><category>neural</category><category>open-science</category><category>SHL_scripts</category><guid>https://laurentperrinet.github.io/sciblog/posts/2017-03-29-testing-comps-pcum.html</guid><pubDate>Tue, 23 May 2017 13:58:37 GMT</pubDate></item><item><title>Predictive coding of motion in an aperture</title><link>https://laurentperrinet.github.io/sciblog/posts/2016-07-16-predictive-coding-of-motion-in-an-aperture.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;After reading the paper &lt;a href="https://laurentperrinet.github.io/sciblog/posts/Motion%20Direction%20Biases%20and%20Decoding%20in%20Human%20Visual%20Cortex"&gt;http://www.jneurosci.org/content/34/37/12601.full&lt;/a&gt; by  Helena X. Wang, Elisha P. Merriam, Jeremy Freeman, and David J. Heeger (The Journal of Neuroscience, 10 September 2014, 34(37): 12601-12615; doi: 10.1523/JNEUROSCI.1034-14.2014), I was interested to test the hypothesis they raise in the discussion section :&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The aperture-inward bias in V1–V3 may reflect spatial interactions between visual motion signals along the path of motion (Raemaekers et al., 2009; Schellekens et al., 2013). Neural responses might have been suppressed when the stimulus could be predicted from the responses of neighboring neurons nearer the location of motion origin, a form of predictive coding (Rao and Ballard, 1999; Lee and Mumford, 2003). Under this hypothesis, spatial interactions between neurons depend on both stimulus motion direction and the neuron's relative RF locations, but the neurons themselves need not be direction selective. Perhaps consistent with this hypothesis, psychophysical sensitivity is enhanced at locations further along the path of motion than at motion origin (van Doorn and Koenderink, 1984; Verghese et al., 1999).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Concerning the origins of aperture-inward bias, I want to test an alternative possibility. In some recent modeling work:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Laurent Perrinet, Guillaume S. Masson. Motion-based prediction is sufficient to solve the aperture problem. Neural Computation, 24(10):2726--50, 2012 &lt;a href="http://invibe.net/LaurentPerrinet/Publications/Perrinet12pred"&gt;http://invibe.net/LaurentPerrinet/Publications/Perrinet12pred&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was surprised to observe a similar behavior: the trailing edge was exhibiting a stronger activation (i. e. higher precision revealed by a lower variance in this probabilistic model) while I would have thought intuitively the leading edge would be more informative. In retrospect, it made sense in a motion-based prediction algorithm as information from the leading edge may propagate in more directions (135° for a 45° bar) than in the trailing edge (45°, that is a factor of 3 here). While we made this prediction we did not have any evidence for it.&lt;/p&gt;
&lt;p&gt;In this script the predictive coding is done using the &lt;code&gt;MotionParticles&lt;/code&gt; package and for a &lt;a href="https://laurentperrinet.github.io/sciblog/posts/motion%20texture"&gt;http://motionclouds.invibe.net/&lt;/a&gt; within a disk aperture.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2016-07-16-predictive-coding-of-motion-in-an-aperture.html"&gt;Read more…&lt;/a&gt; (26 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>LogGabor</category><category>motion</category><category>motionclouds</category><category>neural</category><category>open-science</category><category>predictive coding</category><guid>https://laurentperrinet.github.io/sciblog/posts/2016-07-16-predictive-coding-of-motion-in-an-aperture.html</guid><pubDate>Sat, 16 Jul 2016 05:58:24 GMT</pubDate></item><item><title>compiling notebooks into a report</title><link>https://laurentperrinet.github.io/sciblog/posts/2016-06-25-compiling-notebooks-into-a-report.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;For a master's project in computational neuroscience, we adopted a quite novel workflow to go all the steps from the learning of the small steps to the wrtiting of the final thesis. Though we were flexible in our method during the 6 months of this work, a simple workflow emerged that I describe here.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2016-06-26_thesis.png" alt="Compiling a set of notebook to a LaTeX document."&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2016-06-25-compiling-notebooks-into-a-report.html"&gt;Read more…&lt;/a&gt; (25 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>ipython</category><category>neural</category><category>open-science</category><guid>https://laurentperrinet.github.io/sciblog/posts/2016-06-25-compiling-notebooks-into-a-report.html</guid><pubDate>Sat, 25 Jun 2016 05:58:24 GMT</pubDate></item></channel></rss>