<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook (Posts about orientation)</title><link>https://laurentperrinet.github.io/sciblog/</link><description></description><atom:link href="https://laurentperrinet.github.io/sciblog/categories/orientation.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Wed, 08 Jan 2020 20:49:50 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>2020-01-08 changing the global phase of a Motion Cloud</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-01-08-changing-the-global-phase-of-a-motion-cloud.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Blah blah blah&lt;/p&gt;
&lt;p&gt;Emphasis, aka italics, with &lt;em&gt;asterisks&lt;/em&gt; or &lt;em&gt;underscores&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Strong emphasis, aka bold, with &lt;strong&gt;asterisks&lt;/strong&gt; or &lt;strong&gt;underscores&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Combined emphasis with &lt;strong&gt;asterisks and &lt;em&gt;underscores&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Strikethrough uses two tildes. &lt;del&gt;Scratch this.&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.google.com"&gt;I'm an inline-style link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.google.com" title="Google's Homepage"&gt;I'm an inline-style link with title&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.mozilla.org"&gt;I'm a reference-style link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/blob/master/LICENSE"&gt;I'm a relative reference to a repository file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://slashdot.org"&gt;You can use numbers for reference-style link definitions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Or leave it empty and use the &lt;a href="http://www.reddit.com"&gt;link text itself&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;URLs and URLs in angle brackets will automatically get turned into links. 
&lt;a href="http://www.example.com"&gt;http://www.example.com&lt;/a&gt; or &lt;a href="http://www.example.com"&gt;http://www.example.com&lt;/a&gt; and sometimes 
example.com (but not on Github, for example).&lt;/p&gt;
&lt;p&gt;Some text to show that the reference links can follow later.&lt;/p&gt;
&lt;p&gt;Here's our logo (hover to see the title text):&lt;/p&gt;
&lt;p&gt;Inline-style: 
&lt;img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;Reference-style: 
&lt;img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"&gt;&lt;/p&gt;
&lt;p&gt;Inline &lt;code&gt;code&lt;/code&gt; has &lt;code&gt;back-ticks around&lt;/code&gt; it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"JavaScript syntax highlighting"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nx"&gt;alert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"Python syntax highlighting"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;No language indicated, so no syntax highlighting. 
But let's throw in a &amp;lt;b&amp;gt;tag&amp;lt;/b&amp;gt;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Colons can be used to align columns.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Tables&lt;/th&gt;
&lt;th style="text-align:center"&gt;Are&lt;/th&gt;
&lt;th style="text-align:right"&gt;Cool&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;col 3 is&lt;/td&gt;
&lt;td style="text-align:center"&gt;right-aligned&lt;/td&gt;
&lt;td style="text-align:right"&gt;$1600&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;col 2 is&lt;/td&gt;
&lt;td style="text-align:center"&gt;centered&lt;/td&gt;
&lt;td style="text-align:right"&gt;$12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;zebra stripes&lt;/td&gt;
&lt;td style="text-align:center"&gt;are neat&lt;/td&gt;
&lt;td style="text-align:right"&gt;$1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don't need to make the 
raw Markdown line up prettily. You can also use inline Markdown.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Markdown&lt;/th&gt;
&lt;th&gt;Less&lt;/th&gt;
&lt;th&gt;Pretty&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Still&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;renders&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;nicely&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;&lt;p&gt;Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Quote break.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can &lt;em&gt;put&lt;/em&gt; &lt;strong&gt;Markdown&lt;/strong&gt; into a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-01-08-changing-the-global-phase-of-a-motion-cloud.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>motionclouds</category><category>orientation</category><category>psychophysics</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-01-08-changing-the-global-phase-of-a-motion-cloud.html</guid><pubDate>Wed, 08 Jan 2020 20:43:42 GMT</pubDate></item><item><title>Role of gamma correction in Sparse coding</title><link>https://laurentperrinet.github.io/sciblog/posts/2019-11-28-role-of-gamma-correction-in-sparse-coding.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I have previously shown a &lt;a href="https://github.com/bicv/SparseEdges"&gt;python implementation&lt;/a&gt; which allows for the extraction a sparse set of edges from an image. We were using the raw luminance as the input to the algorithm. What happens if you use &lt;a href="https://en.wikipedia.org/wiki/Gamma_correction"&gt;gamma correction&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2015-05-22-a-hitchhiker-guide-to-matching-pursuit/gamma.png" alt="albert on gamma"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Results : for this particular image, we checked that using the luminance ($\gamma \approx 1$) is the correct choice. The outcome is that &lt;em&gt;gamma correction may improve coding, but only slightly&lt;/em&gt;. In the figure below, we plot as a function of gamma the final energy of the error and the perceptually relevant measure of &lt;a href="https://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html"&gt;structural simailarity&lt;/a&gt; :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2015-05-22-a-hitchhiker-guide-to-matching-pursuit/gamma_results.png" alt="albert on gamma"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This may not be the case for other types of images which would justify an image-by-image local gain control.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;more information on sparse coding is exposed in the following book chapter (see also &lt;a href="https://laurentperrinet.github.io/publication/perrinet-15-bicv"&gt;https://laurentperrinet.github.io/publication/perrinet-15-bicv&lt;/a&gt; ):&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;bibtex
@inbook{Perrinet15bicv,
    title = {Sparse models},
    author = {Perrinet, Laurent U.},
    booktitle = {Biologically-inspired Computer Vision},
    chapter = {13},
    editor = {Keil, Matthias and Crist\'{o}bal, Gabriel and Perrinet, Laurent U.},
    publisher = {Wiley, New-York},
    year = {2015}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2019-11-28-role-of-gamma-correction-in-sparse-coding.html"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>LogGabor</category><category>machine-learning</category><category>orientation</category><category>SLIP</category><category>sparse</category><guid>https://laurentperrinet.github.io/sciblog/posts/2019-11-28-role-of-gamma-correction-in-sparse-coding.html</guid><pubDate>Thu, 28 Nov 2019 11:46:24 GMT</pubDate></item><item><title>Origins of the Von Mises distribution</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-12-23-origins-of-the-von-mises-distribution.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to check if the &lt;a href="https://en.wikipedia.org/wiki/Von_Mises_distribution"&gt;Von Mises distribution&lt;/a&gt; is the &lt;em&gt;a priori&lt;/em&gt; choice to make when handling polar coordinates.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-12-23-origins-of-the-von-mises-distribution.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>math</category><category>neural</category><category>orientation</category><category>sparse</category><category>v1</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-12-23-origins-of-the-von-mises-distribution.html</guid><pubDate>Sun, 23 Dec 2018 19:31:22 GMT</pubDate></item><item><title>Statistics of the natural input to a ring model</title><link>https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;figure&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2018-11-05_Ring_input.png"&gt;&lt;/figure&gt; &lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;What does the input to a population of neurons in the primary visual cortex look like? In this post, we will try to have a feeling of the structure and statistics of the natural input to such a "ring" model.&lt;/p&gt;
&lt;p&gt;This notebook explores this question using a retina-like temporal filtering and oriented Gabor-like filters. It produces this polar plot of the instantaneous energy in the different orientations for a natural movie :&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;video controls autoplay loop src="https://laurentperrinet.github.io/sciblog/files/2018-11-05_Ring_input.mp4" width="61.8%/"&gt; &lt;/video&gt;&lt;/center&gt;
&lt;br&gt;

&lt;p&gt;One observes different striking features in the structure of this input to populations of V1 neurons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input is sparse: often, a few orientations dominate - the shape of these components (bandwidth) seem to be similar,&lt;/li&gt;
&lt;li&gt;there are many "switches": at some moments, the representations flips to another. This is due to cuts in the movie (changes from one scene to the other for instance). In a more realistic setting where we would add eye movements, these switches should also happen during saccades (but is there any knowledge of the occurence of the switch by the visual system?),&lt;/li&gt;
&lt;li&gt;between switches, there is some coherence in amplitude (a component will slowly change its energy) but also in time (a component is more likely to have a ghradually changing oriantation, for instance when the scene rotates).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structure is specific to the structure of natural images and to the way they transform (translations, rotations, zooms due to the motion and deformation of visual objects). This is certainly incorporated as a "prior" information in the structure of the visual cortex. As to know how and where this is implemented is an open scientific question.&lt;/p&gt;
&lt;p&gt;This is joint work with &lt;a href="https://laurentperrinet.github.io/authors/hugo-ladret"&gt;Hugo Ladret&lt;/a&gt;.
&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html"&gt;Read more…&lt;/a&gt; (597 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>LogGabor</category><category>machine-learning</category><category>motion</category><category>motionclouds</category><category>neural</category><category>numpy</category><category>orientation</category><category>psychophysics</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html</guid><pubDate>Mon, 05 Nov 2018 14:41:07 GMT</pubDate></item><item><title>IRM clouds</title><link>https://laurentperrinet.github.io/sciblog/posts/2016-04-06_irm-clouds.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;A feature of MotionClouds is the ability to precisely tune the precision of information  following the principal axes. One which is particularly relevant for the primary visual cortical area of primates (area V1) is to tune the otirentation mean and bandwidth.&lt;/p&gt;
&lt;h3 id="Studying-the-role-of-contrast-in-V1-using-MotionClouds"&gt;Studying the role of contrast in V1 using MotionClouds&lt;a class="anchor-link" href="https://laurentperrinet.github.io/sciblog/posts/2016-04-06_irm-clouds.html#Studying-the-role-of-contrast-in-V1-using-MotionClouds"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2016-04-06_irm-clouds.html"&gt;Read more…&lt;/a&gt; (39 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>area-V1</category><category>experiment</category><category>motionclouds</category><category>orientation</category><category>psychophysics</category><guid>https://laurentperrinet.github.io/sciblog/posts/2016-04-06_irm-clouds.html</guid><pubDate>Wed, 06 Apr 2016 13:52:01 GMT</pubDate></item><item><title>Recruiting different population ratios in V1 using orientation components: defining a protocol</title><link>https://laurentperrinet.github.io/sciblog/posts/2014-12-10_orientation_protocol.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;A feature of MotionClouds is the ability to precisely tune the precision of information  following the principal axes. One which is particularly relevant for the primary visual cortical area of primates (area V1) is to tune the otirentation mean and bandwidth.&lt;/p&gt;
&lt;p&gt;This is part of a larger study to tune &lt;a href="http://motionclouds.invibe.net/posts/orientation.html"&gt;orientation bandwidth&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="summary-of-the-electro-physiology-protocol"&gt;summary of the electro-physiology protocol&lt;a class="anchor-link" href="https://laurentperrinet.github.io/sciblog/posts/2014-12-10_orientation_protocol.html#summary-of-the-electro-physiology-protocol"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2014-12-10_orientation_protocol.html"&gt;Read more…&lt;/a&gt; (42 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>area-V1</category><category>experiment</category><category>motionclouds</category><category>OBV1</category><category>orientation</category><guid>https://laurentperrinet.github.io/sciblog/posts/2014-12-10_orientation_protocol.html</guid><pubDate>Wed, 10 Dec 2014 09:56:20 GMT</pubDate></item><item><title>Recruiting different population ratios in V1 using orientation components</title><link>https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;A feature of MotionClouds is the ability to precisely tune the precision of information  following the principal axes. One which is particularly relevant for the primary visual cortical area of primates (area V1) is to tune the orientation mean and bandwidth.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation.html"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>area-V1</category><category>experiment</category><category>motionclouds</category><category>OBV1</category><category>orientation</category><guid>https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation.html</guid><pubDate>Mon, 10 Nov 2014 09:56:20 GMT</pubDate></item><item><title>Recruiting different population ratios in V1 using orientation components: a biphoton study</title><link>https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation_biphoton.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;A feature of MotionClouds is the ability to precisely tune the precision of information  following the principal axes. One which is particularly relevant for the primary visual cortical area of primates (area V1) is to tune the otirentation mean and bandwidth.&lt;/p&gt;
&lt;p&gt;To install the necessary libraries, check out &lt;a href="https://github.com/NeuralEnsemble/MotionClouds/blob/master/README.md"&gt;the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="summary-of-the-biphoton-protocol"&gt;summary of the biphoton protocol&lt;a class="anchor-link" href="https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation_biphoton.html#summary-of-the-biphoton-protocol"&gt;¶&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For the biphoton experiment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The refresh rate of the screen is 70Hz and stimulation for 5 times 1s, which makes 350 images.&lt;/li&gt;
&lt;li&gt;for the spatial frequency 0.125 cyc/deg is optimal (between 0.01 and 0.16).&lt;/li&gt;
&lt;li&gt;for the temporal frequency 2 cyc/sec is optimal (between 0.8 and 4 sic/sec), we manipulate $B_V$ to get a qualitative estimate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation_biphoton.html"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>area-V1</category><category>experiment</category><category>motionclouds</category><category>OBV1</category><category>orientation</category><guid>https://laurentperrinet.github.io/sciblog/posts/2014-11-10_orientation_biphoton.html</guid><pubDate>Mon, 10 Nov 2014 09:56:20 GMT</pubDate></item></channel></rss>