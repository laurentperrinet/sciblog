<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Master M2 Sciences | Scientific logbook</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://laurentperrinet.github.io/sciblog/posts/2010-10-20-Master-M2-Sciences.html">
<link rel="icon" href="favicon.ico" sizes="16x16">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    },
    displayAlign: 'center', // Change this to 'center' to center equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><meta name="author" content="Laurent Perrinet">
<link rel="prev" href="2010-10-09-nous.html" title="nous" type="text/html">
<link rel="next" href="2010-10-27-installing-python-and-its-components.html" title="installing python and its components" type="text/html">
<meta property="og:site_name" content="Scientific logbook">
<meta property="og:title" content="Master M2 Sciences">
<meta property="og:url" content="https://laurentperrinet.github.io/sciblog/posts/2010-10-20-Master-M2-Sciences.html">
<meta property="og:description" content="Neurosciences Computationelles : émergence dans des réseaux d'information

Laurent Perrinet

System Message: ERROR/3 (&lt;string&gt;, line 11)
Malformed table.

+--------------------------------------------">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2010-10-20T13:36:57+02:00">
<meta property="article:tag" content="computationalneuroscience">
<meta property="article:tag" content="sciblog">
<meta property="article:tag" content="talks">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
<!-- Custom search -->
<form method="get" id="search" action="//duckduckgo.com/" class="navbar-form pull-left">
<input type="hidden" name="sites" value="https://laurentperrinet.github.io/sciblog/"><input type="hidden" name="k8" value="#444444"><input type="hidden" name="k9" value="#D51920"><input type="hidden" name="kt" value="h"><input type="text" name="q" maxlength="255" placeholder="Search…" class="span2" style="margin-top: 4px;"><input type="submit" value="DuckDuckGo Search" style="visibility: hidden;">
</form>
<!-- End of custom search -->

            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="https://laurentperrinet.github.io/sciblog/">

            <span id="blog-title">Scientific logbook</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="2010-10-20-Master-M2-Sciences.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../index.html" class="nav-link">Home</a>
                </li>
<li class="nav-item">
<a href="../archive.html" class="nav-link">Archives</a>
                </li>
<li class="nav-item">
<a href="../categories/index.html" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../rss.xml" class="nav-link">RSS</a>
                </li>
<li class="nav-item">
<a href="http://laurentperrinet.github.io" class="nav-link">About my research</a>
                </li>
<li class="nav-item">
<a href="https://twitter.com/laurentperrinet" class="nav-link">Twitter</a>
                </li>
<li class="nav-item">
<a href="https://github.com/laurentperrinet" class="nav-link">Github</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Master M2 Sciences</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Laurent Perrinet
            </span></p>
            <p class="dateline">
            <a href="#" rel="bookmark">
            <time class="published dt-published" datetime="2010-10-20T13:36:57+02:00" itemprop="datePublished" title="2010-10-20 13:36">2010-10-20 13:36</time></a>
            </p>
                <p class="commentline">
        
    <a href="2010-10-20-Master-M2-Sciences.html#disqus_thread" data-disqus-identifier="cache/posts/2010-10-20-Master-M2-Sciences.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="2010-10-20-Master-M2-Sciences.rst" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<!-- TEASER_END -->
<div class="section" id="neurosciences-computationelles-emergence-dans-des-reseaux-d-information">
<h2>Neurosciences Computationelles : émergence dans des réseaux d'information</h2>
<p><img alt="http://2010.neurocomp.fr/images/neurocomplog.png" src="http://2010.neurocomp.fr/images/neurocomplog.png"></p>
<p>Laurent Perrinet</p>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">&lt;string&gt;</tt>, line 11)</p>
<p>Malformed table.</p>
<pre class="literal-block">
+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| |https://crac.dsi.cnrs.fr/image/logo_cnrs.gif|    | Laurent Perrinet - Team `InViBe &lt;/LaurentPerrinet/InViBe&gt;`__, `Institut de Neurosciences de la Timone &lt;http://www.int.univ-amu.fr/PERRINET-Laurent&gt;`__ (UMR 7289)   |
|                                                   |  CNRS - Aix-Marseille Université                                                                                                                                    |
|                                                   |  *Researcher*                                                                                                                                                       |
|                                                   |  `https://invibe.net/LaurentPerrinet &lt;https://invibe.net/LaurentPerrinet&gt;`__                                                                                          |
+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</pre>
</div>
<p>Les Neurosciences Computationelles étudient l'émergence dans les réseaux
neuronaux (structure) des fonctions cognitives (fonction). Durant ce
cours, nous allons définir ce paradigme de façon générique avant de
l'appliquer à trois cas particuliers. D'abord nous allons étudier un
exemple de réseau de type Bayesien appliqué à la détection du mouvement
visuel. Ensuite, nous étudierons un exemple de réseau de neurones
abstraits appliqué au codage adaptatif d'images. Enfin, l'étude de
différents types de connectivité dans des réseaux de neurones
impulsionnels réalistes.</p>
<div class="section" id="plan-du-cours">
<h3>Plan du cours</h3>
<ol class="arabic simple">
<li><a class="reference external" href="../LaurentPerrinet/Presentations/10-10-20_M2_MasterSciences/017_Emergence">Neurosciences
computationnelles?</a></li>
<li><a class="reference external" href="../LaurentPerrinet/Presentations/10-10-20_M2_MasterSciences/035_MotionPerception">Une première application fonctionnelle: détection du mouvement
utilisant des modèles
probabilistes.</a></li>
<li><a class="reference external" href="../LaurentPerrinet/Presentations/10-10-20_M2_MasterSciences/060_SparseCoding">Réseaux optimaux pour le calcul de la représentation d'une
image</a></li>
<li><a class="reference external" href="../LaurentPerrinet/Presentations/10-10-20_M2_MasterSciences/070_NeuralCoding">Vers des implantations
neurales...</a></li>
</ol>
<div class="section" id="le-cerveau-est-elle-une-machine-de-turing">
<h4>Le cerveau est-elle une machine de Turing ?</h4>
<p><img alt="http://www.ecs.syr.edu/faculty/fawcett/handouts/webpages/pictures/turingMachine.gif" src="http://www.ecs.syr.edu/faculty/fawcett/handouts/webpages/pictures/turingMachine.gif"></p>
</div>
</div>
<div class="section" id="le-systeme-nerveux-central-quelques-chiffres">
<h3>Le Système Nerveux Central: quelques chiffres</h3>
<ul class="simple">
<li>Cerveau = 100 milliards de neurones (cerveau humain)</li>
<li>2% du poids, 20% de la consommation énergétique (5W)</li>
<li>1mm^3 = 90 000 neurones / 700.000.000 synapses / 4 km d’axone / 0,5
km de dendrites</li>
</ul>
<p><img alt="http://upload.wikimedia.org/wikipedia/commons/c/c2/SnowflakesWilsonBentley.jpg" src="http://upload.wikimedia.org/wikipedia/commons/c/c2/SnowflakesWilsonBentley.jpg"></p>
</div>
<div class="section" id="emergence-dans-des-reseaux-d-information">
<h3>Émergence dans des réseaux d'information</h3>
<ul class="simple">
<li>Approche classique: réductionisme , atomisme</li>
<li><em>computere</em></li>
<li>émergence ( <a class="reference external" href="http://en.wikipedia.org/wiki/Emergence">{en}</a>
<a class="reference external" href="http://fr.wikipedia.org/wiki/Emergence">{fr}</a> )</li>
</ul>
<div class="section" id="neurosciences-computationelles">
<h4>Neurosciences Computationelles?</h4>
<p><img alt="http://parasol.tamu.edu/~neilien/research/neurons.jpg" src="http://parasol.tamu.edu/~neilien/research/neurons.jpg"></p>
<ul>
<li>
<p class="first">Les <strong>Neurosciences Computationelles</strong> étudient l'émergence dans les
<em>réseaux neuronaux</em> (structure) des <em>fonctions cognitives</em> (fonction)
en termes de <em>traitement d'information</em> (méthode).</p>
<p><img alt="alert" src="https://invibe.net/moin_static196/moniker/img/alert.png"> ne pas confondre Neurosciences Computationelles et
NeuroInformatique !</p>
</li>
<li>
<p class="first">convergence interdisciplinaire de :</p>
<ul class="simple">
<li>neurosciences (physiologie, psychophysique)</li>
<li>mathématiques (EDP, physique statistique, probabilités et
statistiques, calcul stochastique, théorie des graphes, physique
statistique, ...)</li>
<li>informatique (théorie de l'information, théorie computationnelle,
simulation de modèles)</li>
</ul>
</li>
<li>
<p class="first">applications à la compréhension de la biologie, à répondre aux
pathologies + création de nouveaux paradigmes computationnels</p>
</li>
</ul>
</div>
<div class="section" id="histoire-des-neurosciences-computationelles">
<h4>Histoire des Neurosciences Computationelles</h4>
<ul class="simple">
<li>suit d'abord l'histoire des neurosciences
<img alt="http://upload.wikimedia.org/wikipedia/commons/7/75/Duck_of_Vaucanson.jpg" src="http://upload.wikimedia.org/wikipedia/commons/7/75/Duck_of_Vaucanson.jpg">
</li>
<li>1930 : Turing
<img alt="http://longstreet.typepad.com/.a/6a00d83542d51e69e20120a5d6fc90970c-500wi" src="http://longstreet.typepad.com/.a/6a00d83542d51e69e20120a5d6fc90970c-500wi">
</li>
<li>1950 : Hebb / von Neumann</li>
<li>1950 : réseaux de neurones, parallelisme (PDP, Rosenblatt)</li>
<li>1980 : étude des systèmes complexes, arrivée de l'émergence (Amari,
Grossberg, Kohonen, Hopfield, physique statistique), premières
définitions des Neurosciences Computationelles (congrès de Carmel)</li>
<li>1990 : machine learning (NIPS)</li>
<li>2000 : machines hybrides, science de la complexité (probabilités),
approche système (explosion du volume de données)
<img alt="http://people.csail.mit.edu/koen/wholeBrainAtlasMesh.gif" src="http://people.csail.mit.edu/koen/wholeBrainAtlasMesh.gif">
</li>
<li>2010 : ...</li>
</ul>
</div>
<div class="section" id="niveaux-d-analyse-de-david-marr">
<h4>Niveaux d'analyse de David Marr</h4>
<table border="1" class="docutils">
<colgroup>
<col width="22%">
<col width="43%">
<col width="35%">
</colgroup>
<tbody valign="top">
<tr>
<td>
<strong>fonction</strong> / définition</td>
<td>
<strong>algorithmique</strong> / méthode</td>
<td>
<strong>hardware</strong> / structure, support neural</td>
</tr>
<tr>
<td>analyse spectrale</td>
<td>décomposition de Fourier</td>
<td>FFT</td>
</tr>
<tr>
<td>cognition et comportement (modèles de prise de décision; conditionnement classique; conditionnement opérant; apprentissage par réenforcement; neuroéconomie)</td>
<td>Traitement de l'information (traitement sensoriel; filtres linéaires et champs récepteurs; estimation des champs récepteurs; détecteurs de contour; modèle de Hubel et Wiesel; statistiques des images naturelles; théorie de l'information; analyse en composantes indépendante; décodage neuronale; codage par population)</td>
<td>Dynamique et mécanismes (biophysique d'un neurone; génération de potentiels d'action; réseaux de neurones feedforward et récurrent; réseaux attracteurs; fonctions d'énergie, énergie de Liapounov; apprentissage et plasticité synaptique; mémoires associatives)</td>
</tr>
<tr>
<td>Modèles des systèmes neuronaux : La perception visuelle, système vestibulaire, contrôle oculomoteur, contrôle des membres, prise de décision.</td>
<td>Théorie des réseaux neuronaux : Codage neuronaux, apprentissage supervisé et non supervisé, apprentissage par renforcement.</td>
<td>Modèles de neurones : Modèles de membrane, potentiels d’action, équation Hodgkin-Huxley, modèles de compartiments, canaux, synapses.</td>
</tr>
<tr>
<td><em>suivi d'objet</em></td>
<td><em>détection d'objet</em></td>
<td><em>circuit V1-MT/MST-FEF</em></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="suivi-d-objet-l-exemple-de-l-occular-following-response">
<h4>Suivi d'objet: l'exemple de l'Occular Following Response</h4>
<table border="1" class="docutils">
<colgroup><col width="100%"></colgroup>
<tbody valign="top">
<tr>
<td>
<img alt="grating_contrast" src="https://invibe.net//Figures/Perrinet07neurocomp/FigureZero?action=AttachFile&amp;do=get&amp;target=grating_contrast.gif"><img alt="grating_size" src="https://invibe.net//Figures/Perrinet07neurocomp/FigureZero?action=AttachFile&amp;do=get&amp;target=grating_size.gif"><img alt="full_field_barberpole" src="https://invibe.net//Figures/Perrinet07neurocomp/FigureZero?action=AttachFile&amp;do=get&amp;target=full_field_barberpole.gif">
</td>
</tr>
<tr>
<td>
<a class="reference external" href="../LaurentPerrinet/Figures/Perrinet07neurocomp/FigureZero">Figure</a> <strong>Stimuli used for testing OFR.</strong> <em>(Left) Grating in a disk aperture with varying contrast and</em>(Middle)*with varying diameters.*(Right) Barberpole.</td>
</tr>
</tbody>
</table>
<p><img alt="https://invibe.net/LaurentPerrinet/Figures/Perrinet07neurocomp/FigureUn?action=AttachFile&amp;do=get&amp;target=summary.png" src="https://invibe.net/LaurentPerrinet/Figures/Perrinet07neurocomp/FigureUn?action=AttachFile&amp;do=get&amp;target=summary.png"></p>
<p><img alt="Figures/Perrinet04tauc/FigureUn/inverse.png" src="https://invibe.net//Figures/Perrinet04tauc/FigureUn?action=AttachFile&amp;do=get&amp;target=inverse.png"></p>
</div>
<div class="section" id="modeles-probabilistes">
<h4>Modèles Probabilistes</h4>
<p><img alt="model_simple.png" src="https://invibe.net//Presentations/10-10-20_M2_MasterSciences/050_ProbabilisticModels?action=AttachFile&amp;do=get&amp;target=model_simple.png"></p>
</div>
</div>
<div class="section" id="avantages-des-representations-probabilitistes">
<h3>Avantages des représentations probabilitistes</h3>
<ol class="arabic simple">
<li>Règles de calcul probabiliste / lien avec la théorie de l'information</li>
<li>Le modèle génératif (<em>vraissemblance</em>) permet de baser inférence
(échelle temporelle du codage) et apprentissage (échelle temporelle
de l'adaptation)</li>
<li>Modèles hiérarchiques</li>
<li>Réseaux bayesiens</li>
</ol>
</div>
<div class="section" id="la-regle-de-bayes">
<h3>La règle de Bayes</h3>
<ol class="arabic simple">
<li>$P( \vec{V} | {\bf I} ) \propto P( {\bf I} | \vec{V} ). P(
\vec{V} )$ :<ol class="arabic">
<li>on veut calculer la probabilité <em>a posteriori</em>,</li>
<li>Le modèle génératif permet de définir la probabilité de
<em>vraissemblance</em> de tous les modèles directs,</li>
<li>On introduit un prior (ici perceptif) modulant cette probabilité.</li>
</ol>
</li>
</ol>
<div class="section" id="de-la-mesure-aux-probabilites">
<h4>De la mesure aux probabilités</h4>
<ul class="simple">
<li>Soit $\mathbf{I}$ une image contenant du mouvement</li>
<li>La meilleure estimation de la vitesse de translation est:
$$\vec{v}^\ast = E( \vec{V} | \mathbf{I} ) = \int \vec{V} dP(
\vec{v} | \mathbf{I} )$$</li>
</ul>
<p><img alt="../050_ProbabilisticModels/model_simple.png" src="https://invibe.net//Presentations/10-10-20_M2_MasterSciences/050_ProbabilisticModels?action=AttachFile&amp;do=get&amp;target=model_simple.png"></p>
</div>
</div>
<div class="section" id="bayes">
<h3>Bayes</h3>
<ul class="simple">
<li>$$P( \vec{V} | {\bf I} ) \propto P( {\bf I} | \vec{V} ). P(
\vec{V} )$$</li>
</ul>
</div>
<div class="section" id="un-modele-du-mouvement">
<h3>Un modèle du mouvement</h3>
<ul class="simple">
<li>Connaissant $\vec{V}$, on estime que $\mathbf{I}(\vec{x},t)
\approx \mathbf{I}(\vec{x} - \vec{V} . dt ,t-dt)$</li>
<li>$$P( {\bf I} | \vec{V} ) \propto \exp(- \frac{C<sup>2 .
\mathcal{T}(\mathbf{I}_{100})</sup>2}{2.\sigma_m^2})$$<ul>
<li>avec $\mathcal{T}(\mathbf{I}_{100}) = \|
\mathbf{I}_{100}(\vec{x},t) - \mathbf{I}_{100}(\vec{x} -
\vec{V} . dt ,t-dt) \|$</li>
<li>Son contraste est $C$ par rapport à une référence:
$\mathbf{I}=C.\mathbf{I}_{100}$</li>
</ul>
</li>
<li>Hypothèse quadratique: $$P( {\bf I} | \vec{V} ) = \mathcal{N}(
\vec{V_m} , \sigma_m )$$</li>
</ul>
</div>
<div class="section" id="inclusion-d-un-prior-basse-vitesse">
<h3>inclusion d'un prior basse vitesse</h3>
<ul class="simple">
<li>$$P( \vec{V} ) = \mathcal{N}( 0 , \sigma_p )$$</li>
<li>On en déduit: $$ P( \vec{V} | \mathbf{I} ) \propto \exp(-
\frac{C<sup>2 . \| \vec{V}-\vec{V_m}
\|</sup>2}{2.\sigma_m<sup>2 } - \frac{ \| \vec{V}
\|</sup>2}{2.\sigma_p^2 })$$</li>
</ul>
<p><img alt="naka_rushton.png" src="https://invibe.net//Presentations/10-10-20_M2_MasterSciences/055_ProbabilisticModelsForMotionPerception?action=AttachFile&amp;do=get&amp;target=naka_rushton.png"></p>
</div>
<div class="section" id="naka-rushton">
<h3>Naka-Rushton</h3>
<ul class="simple">
<li>On définit le gain $$\gamma (C) = \frac{ \vec{V}(C)}{ \vec{V_m}
}$$</li>
<li>On trouve: $$\gamma (C) \propto
\frac{C<sup>2}{C_{50}</sup>2+C^2}$} with $C_{50} \propto
\frac{\sigma_p}{\sigma_m}$$</li>
</ul>
<div class="section" id="integration-d-informations-independantes">
<h4>intégration d'informations indépendantes</h4>
<p><img alt="https://invibe.net/LaurentPerrinet/Figures/Perrinet08areadne/FigureDeux?action=AttachFile&amp;do=get&amp;target=model_rog.png" src="https://invibe.net/LaurentPerrinet/Figures/Perrinet08areadne/FigureDeux?action=AttachFile&amp;do=get&amp;target=model_rog.png"></p>
<ul>
<li>
<p class="first">$\mathcal{N} (\vec{V}_
\bf n
C
\bf n
)=\frac{1}{\sqrt{det(2 \pi C
\bf n
)}}.exp(\frac{1}{2} (\vec{V}-\vec{V}_
\bf n
)<sup>T C{{\bf n}}</sup>{-1} (\vec{V} - \vec{V}_
\bf n
)$</p>
</li>
<li>
<p class="first">avec $C_</p>
<p>\bf n</p>
<p>$ donné par</p>
<pre class="literal-block">
\begin{eqnarray*}%
\left( \begin{array}{ccc}%
\cos(\theta) &amp; -\sin(\theta) \\%
\sin(\theta) &amp; \cos(\theta)%
\end{array} \right)%
\left( \begin{array}{ccc}%
\sigma_{{\bf n}}^2 &amp; 0 \\%
0 &amp; \sigma_2^2%
\end{array} \right)%
\end{eqnarray*}%
</pre>
</li>
<li>
<p class="first">Indépendence des bruits de mesure: $ P( \vec{V} | \mathbf{I} ) =
\Pi_</p>
<p>\bf n</p>
<p>P( \vec{V} | \mathbf{I} , {\bf n})=\mathcal{N} (\vec{v}_m,C)$
avec :</p>
<pre class="literal-block">
\begin{eqnarray*}
\left\{
\begin{array}{rcl}
C^{-1}              &amp;=&amp; \sum C^{-1}_{{\bf n}}\\
C^{-1} . \vec{v}_m &amp;=&amp; \sum C^{-1}_{{\bf n}} \vec{v}_{{\bf n}}
 \end{array}
 \right.
\end{eqnarray*}
</pre>
</li>
<li>
<p class="first">d'où</p>
<pre class="literal-block">
\begin{eqnarray*}%
C_{{\bf n}}^{-1} = %
\left( \begin{array}{ccc}
\cos(\theta) &amp; \sin(\theta) \\
-\sin(\theta) &amp; \cos(\theta)
\end{array} \right)
\left( \begin{array}{ccc}
\sigma_{{\bf n}}^{-2}  &amp; 0 \\
0 &amp; \sigma_2^{-2}
\end{array} \right)
\end{eqnarray*}
</pre>
</li>
</ul>
</div>
<div class="section" id="integration-spatio-temporelle">
<h4>intégration spatio-temporelle</h4>
<p><img alt="https://invibe.net/LaurentPerrinet/Figures/Perrinet08areadne/FigureTrois?action=AttachFile&amp;do=get&amp;target=fit_BRF.png" src="https://invibe.net/LaurentPerrinet/Figures/Perrinet08areadne/FigureTrois?action=AttachFile&amp;do=get&amp;target=fit_BRF.png"></p>
<ul>
<li>
<p class="first">intégration sur la surface d'un disque:</p>
<pre class="literal-block">
\begin{eqnarray*}%
\gamma(d) = \frac{C^2}{C_e^2} . \frac{ 1- \exp(-\frac{d^2}{2.\omega^2}) }{ 1 + \frac{C^2}{C_e^2}.(1- \exp(-\frac{d^2}{2.\omega^2})) }%
\end{eqnarray*}%
</pre>
</li>
<li>
<p class="first">avec un champ inhibiteur</p>
<pre class="literal-block">
\begin{eqnarray*}
\gamma(d_c) = \frac{ \frac{C^2}{C_e^2} . g_e }{ 1 + \frac{C^2}{C_i^2}. g_i  + \frac{C^2}{C_e^2}. g_e}
\mbox{ with }
\left\{
\begin{array}{rcl}
g_e              &amp;=&amp; 1- \exp(-\frac{d_c^2}{2.\omega^2}) \\
g_i &amp;=&amp; 1- \exp(-\frac{d_c^2}{2.\omega_i^2})
 \end{array}
 \right.
\end{eqnarray*}
</pre>
</li>
<li>
<p class="first">extensible à d'autres formes d'intégration</p>
</li>
</ul>
<p><img alt="Animation of the formation of RFs during aSSC learning" src="https://invibe.net//SparseHebbianLearning?action=AttachFile&amp;do=get&amp;target=ssc.gif"></p>
</div>
<div class="section" id="des-probas-a-une-definition-du-cout-neural">
<h4>Des probas à une définition du coût neural</h4>
<p>$$ {\bf I} = \Psi \cdot \vec{c} + \vec{\nu} $$</p>
<p>$$ \mathcal{C}( {\bf I} | \vec{c} , \Psi) = -\log P( {\bf I} |
\vec{o} , \Psi ) $$ $$ \mathcal{C}( {\bf I} | \vec{c} , \Psi ) =
\log Z + \frac{1}{2\sigma_n<sup>2} \| {\bf I} - \Psi \cdot
\vec{c} \|</sup>2 - \sum_i \log P(c_i | \Psi)$$ $$ \mathcal{C}(
{\bf I} | \vec{c} , \Psi ) = \log Z +
\frac{1}{2\sigma_n<sup>2} \| {\bf I} - \Psi \cdot \vec{c}
\|</sup>2 - \lambda \| \vec{c} \|_0$$</p>
<ul class="simple">
<li>un problème inverse insoluble (NP-complet). Soyons gourmants!</li>
</ul>
</div>
<div class="section" id="du-cout-neural-au-code-neuronal">
<h4>du coût neural au code neuronal</h4>
</div>
</div>
<div class="section" id="apprentissage-par-descente-de-gradient">
<h3>apprentissage par descente de gradient</h3>
<ul class="simple">
<li>connaissant le $\vec{c}$ optimal, $\forall i, \Psi_{i} $ devient
$ \Psi_{i} + \eta c_{i} ({\bf I} - \Psi\cdot\vec{c}) $</li>
</ul>
</div>
<div class="section" id="codage-par-matching-pursuit">
<h3>codage par Matching Pursuit</h3>
<ol class="arabic simple">
<li>pour un $\Psi$ donné, on choisit $ i^\ast =
\mbox{<a class="reference external" href="../LaurentPerrinet/ArgMax">ArgMax</a>}_i (\rho_i)$ avec
$\rho_i = &lt;\frac
\bf I
{\| {\bf I} \|} , \frac{ \Psi_i}{\| \Psi_i\|} &gt; $</li>
<li>comme $ {\bf I} = a_{i<sup>\ast} \dico_{i</sup>\ast} + \bf{R} $,
utilisons $\bf{R}$ et retournons à <strong>1.</strong>
</li>
</ol>
<p>Pour plus d'informations, voir <a class="reference external" href="../LaurentPerrinet/Publications/Perrinet10shl">Perrinet, Neural Computation
(2010)</a>.</p>
<p><img alt="Figures/Perrinet03ieee/FigureZero/v1_tiger.gif" src="https://invibe.net//Figures/Perrinet03ieee/FigureZero?action=AttachFile&amp;do=get&amp;target=v1_tiger.gif"></p>
<div class="section" id="d-autres-modeles-de-plasticite">
<h4>D'autres modèles de plasticité</h4>
<p><img alt="http://topographica.org/Tutorials/images/oo_or_map.png" src="http://topographica.org/Tutorials/images/oo_or_map.png"></p>
</div>
</div>
<div class="section" id="apprentissage-non-supervise">
<h3>Apprentissage non-supervisé</h3>
<ol class="arabic simple">
<li>Apprentissage Hebbien linéaire (PCA), décorrelation</li>
<li>Réseaux Winner-take-all, clustering</li>
<li>Codes distribués parcimonieux (sparse coding)</li>
</ol>
</div>
<div class="section" id="plasticite-et-cartes-corticales">
<h3>Plasticité et cartes corticales</h3>
<ol class="arabic simple">
<li>Self-organizing maps, Kohonen nets</li>
<li>Modèles de ré-organisation topographique</li>
<li>Apprentissage de sous-variétés</li>
</ol>
<div class="section" id="codage-neural-et-systemes-dynamiques-lineaires">
<h4>Codage Neural et systèmes dynamiques linéaires</h4>
<p><img alt="http://upload.wikimedia.org/wikipedia/en/3/3f/LinDynSysTraceDet.jpg" src="http://upload.wikimedia.org/wikipedia/en/3/3f/LinDynSysTraceDet.jpg"></p>
<ul class="simple">
<li>systèmes dynamiques linéaires $$ \frac{d}{dt} \mathbf{x}(t) =
\mathbf{A} \mathbf{x}(t) $$</li>
<li>Les racines de $det(A- \lambda I)$ sont les valeurs propores de $A$.
Le signe des racines determine la stabilité du système.</li>
<li>Pour 2-dimensions, le polynôme characteristique est de la forme
$\lambda<sup>2-\tau\lambda+\Delta=0$. Les racines sont donc:
$$ \lambda=\frac{\tau \pm \sqrt{\tau</sup>2-4\Delta}}{2}$$</li>
</ul>
<p><img alt="http://upload.wikimedia.org/wikipedia/en/5/55/LinearFields.png" src="http://upload.wikimedia.org/wikipedia/en/5/55/LinearFields.png"></p>
</div>
<div class="section" id="codage-neural-et-systemes-dynamiques-non-lineaires">
<h4>Codage Neural et systèmes dynamiques NON linéaires</h4>
<p><a class="reference external" href="https://invibe.net/LaurentPerrinet/Figures/Voges10neurocomp/FigureTrois"><img alt="Figures/Voges10neurocomp/FigureTrois" src="https://invibe.net//Figures/Voges10neurocomp/FigureTrois?action=AttachFile&amp;do=get&amp;target=Voges10Fig3.jpg"></a></p>
</div>
<div class="section" id="codage-neural-et-systemes-dynamiques-non-lineaires-2">
<h4>Codage Neural et systèmes dynamiques NON linéaires (2)</h4>
<p><a class="reference external" href="https://invibe.net/LaurentPerrinet/Figures/Voges09cosyne/FigureModel"><img alt="Figures/Voges09cosyne/FigureModel" src="https://invibe.net//Figures/Voges09cosyne/FigureModel?action=AttachFile&amp;do=get&amp;target=nicole.jpg"></a></p>
</div>
<div class="section" id="codage-neural-et-systemes-dynamiques-non-lineaires-3">
<h4>Codage Neural et systèmes dynamiques NON linéaires (3)</h4>
<p><a class="reference external" href="https://invibe.net/LaurentPerrinet/Figures/Kremkow10jcns/FigureTrois"><img alt="Figures/Kremkow10jcns/FigureTrois" src="https://invibe.net//Figures/Kremkow10jcns/FigureTrois?action=AttachFile&amp;do=get&amp;target=KremkowFig3.png"></a></p>
</div>
<div class="section" id="des-points-a-retenir">
<h4>Des points à retenir</h4>
<ul class="simple">
<li>Importance de poser (toutes) les hypothèses: a-t-on compris
l'ensemble du signal?</li>
<li>Compréhension de la cognition à différents niveaux d'analyse, à
différentes échelles.</li>
<li>Nous quittons le siècle de l'information. Nous entrons dans le siècle
de la complexité.</li>
</ul>
</div>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../categories/computationalneuroscience.html" rel="tag">computationalneuroscience</a></li>
            <li><a class="tag p-category" href="../categories/sciblog.html" rel="tag">sciblog</a></li>
            <li><a class="tag p-category" href="../categories/talks.html" rel="tag">talks</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="2010-10-09-nous.html" rel="prev" title="nous">Previous post</a>
            </li>
            <li class="next">
                <a href="2010-10-27-installing-python-and-its-components.html" rel="next" title="installing python and its components">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="invibe",
            disqus_url="https://laurentperrinet.github.io/sciblog/posts/2010-10-20-Master-M2-Sciences.html",
        disqus_title="Master M2 Sciences",
        disqus_identifier="cache/posts/2010-10-20-Master-M2-Sciences.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="invibe";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer">
            Contents © 2020 <a href="mailto:laurent.perrinet@univ-amu.fr">Laurent Perrinet</a> -
Powered by <a href="http://getnikola.com">Nikola</a> with <a href="https://themes.getnikola.com/v7/maupassant/">Maupassant</a> theme <br><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/">
<img alt="Creative Commons License BY-NC-SA" style="border-width:0; margin-bottom:12px;" src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"></a>
            
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
