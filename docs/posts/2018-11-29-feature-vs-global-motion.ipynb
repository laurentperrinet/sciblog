{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see a visual scene, there is contribution of the motion of each of the objects that constitute the visual scene into detecting its global motion.\n",
    "In particular, it is debatable to know which weight individual features, such as small objects in the foreground, have into this computation compared to a dense texture-like stimulus, as that of the background for instance.\n",
    "\n",
    "Here, we design a a stimulus where we control independently these two aspects of motions to titrate their relative contribution to the detection of motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><video controls autoplay loop src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_B_V_PSE_NL.mp4\" width=100%/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can you spot the motion ? Is it more going to the upper left or to the upper right?\n",
    "\n",
    "(For a more controlled test, imagine you fixate on the center of the movie.)\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's start with a texture-like stimulus (a [Motion Cloud](https://neuralensemble.github.io/MotionClouds/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:06:58.608090Z",
     "start_time": "2018-01-16T15:06:56.480814Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'trajectory'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import MotionClouds as mc\n",
    "fx, fy, ft = mc.get_grids(mc.N_X, mc.N_Y, mc.N_frame)\n",
    "disk = mc.frequency_radius(fx, fy, ft) < .5\n",
    "\n",
    "mc.figpath = '../files/2018-11-29-feature-vs-global-motion'\n",
    "if not(os.path.isdir(mc.figpath)): os.mkdir(mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some default parameters for the textons used here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(sf_0=0.1, B_sf=0.02, B_theta=np.inf, B_V=2., V_Y=1.)\n",
    "global_contrast = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a dense, stationary noise with a single motion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:14.108893Z",
     "start_time": "2018-01-16T15:06:58.614799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_dense.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_dense'\n",
    "seed = 42\n",
    "mc1 = mc.envelope_gabor(fx, fy, ft, V_X=-1., **opts)\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath): mc.figures(mc1, name_, seed=seed, figpath=mc.figpath)\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can overlay this with a similar motion in the upper right direction, such that one obtains a texture generalizing a [plaid stimulus](https://neuralensemble.github.io/MotionClouds/posts/motion-plaid.html), that we place in a disk to make all directions iso-probable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_plaid.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_plaid'\n",
    "   \n",
    "mc1 = mc.envelope_gabor(fx, fy, ft, V_X=-1., **opts)\n",
    "movie1 = mc.rectif(mc.random_cloud(mc1, seed=seed))\n",
    "mc2 = mc.envelope_gabor(fx, fy, ft, V_X=+1., **opts)\n",
    "movie2 = mc.rectif(mc.random_cloud(mc2, seed=seed+1))\n",
    "    \n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(mc.rectif(movie1+movie2, contrast=global_contrast)*disk + .5*(1-disk), os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information is distributed densely in space and time and the motion energy is distributed toward the 2 upper diagonals: the perceived motion is upward vertical, along the vector average. Note that (especially if you are a well trained psychophysician) you may perceive the 2 components of these motion. Do avoid having this transparent motion, one can lower the ``global_contrast`` scalar.\n",
    "\n",
    "## defining features\n",
    "\n",
    "It is also possible to define a texture where, instead of being dense, the number of \"texton\" used to generate the the texture is relatively sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:33.397357Z",
     "start_time": "2018-01-16T15:07:14.115329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_features.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_features'\n",
    "\n",
    "rho = 1.e-3\n",
    "events = np.random.normal(size=(mc.N_X, mc.N_Y, mc.N_frame))\n",
    "events *= np.random.uniform(size=(mc.N_X, mc.N_Y, mc.N_frame)) < rho\n",
    "\n",
    "mc2 = mc.envelope_gabor(fx, fy, ft, V_X=+1., **opts)\n",
    "movie2 = mc.rectif(mc.random_cloud(mc2, seed=seed+1, events=events))\n",
    "    \n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(movie2, os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we now overlay these 2 components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_overlay_features.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_overlay_features'\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(mc.rectif(movie1+movie2, contrast=global_contrast)*disk + .5*(1-disk), os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to perceive a motion which is mostly on the upper left: dense motion dominates - this seems quite logical as the energy of the features component is lower. Is there a way to weight down this component to obtain a \"Point of Subjective Equality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:56.663100Z",
     "start_time": "2018-01-16T15:07:33.425386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_contrast_PSE.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_contrast_PSE'\n",
    "contrast = .3\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(mc.rectif(contrast*movie1+movie2, contrast=global_contrast)*disk + .5*(1-disk), os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property would be to determine the contrast of PSE for different values for the sparseness of the feature component. (With the obvious result that it is $1$ for a sparseness of $1$, that is, for a dense feature component.)\n",
    "\n",
    "## changing the properties of individual features\n",
    "\n",
    "It is now possible to do the same procedure to compare the relative weight of the properties of the textons in each components. In particular, can \"longer\" features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:56.663100Z",
     "start_time": "2018-01-16T15:07:33.425386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_B_V_features.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_B_V_features'\n",
    "\n",
    "B_V2 = .3 * opts['B_V']\n",
    "opts_ = opts.copy()\n",
    "opts_.update(B_V=B_V2)\n",
    "mc2 = mc.envelope_gabor(fx, fy, ft, V_X=+1., **opts_)\n",
    "movie2 = mc.rectif(mc.random_cloud(mc2, seed=seed+1, events=events))\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(movie2, os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...have a higher weight on the global motion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-16T15:07:56.663100Z",
     "start_time": "2018-01-16T15:07:33.425386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_B_V_PSE.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_ = name + '_B_V_PSE'\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(mc.rectif(movie1+movie2, contrast=global_contrast)*disk + .5*(1-disk), os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it would be interesting to test the evolution of these parameters (contrast, sparseness, precision) to achieve the PSE.\n",
    "\n",
    "One key aspect of local features is that they are overlaid on top of the background. This is easy to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <center><table border=none width=100% height=100%>\n",
       "            <tr> <td width=100%><center><video  loop=\"1\" autoplay=\"1\" controls  src=\"../files/2018-11-29-feature-vs-global-motion/trajectory_B_V_PSE_NL.mp4\" type=\"video/mp4\"  width=100%\\>\n",
       "            </td></tr></table></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def overlay(movie1, movie2, rho, do_linear=False):\n",
    "    if do_linear:\n",
    "        return rho*movie1+(1-rho)*movie2\n",
    "    else:\n",
    "        movie1, movie2 = rho*(2.*movie1-1), 2.*movie2-1\n",
    "        movie = movie1 * (np.abs(movie1) > np.abs(movie2)) + movie2 * (np.abs(movie1) <= np.abs(movie2))\n",
    "        return .5 + .5*movie\n",
    "\n",
    "name_ = name + '_B_V_PSE_NL'\n",
    "if mc.check_if_anim_exist(name_, figpath=mc.figpath):\n",
    "    mc.anim_save(mc.rectif(overlay(movie1, movie2, rho=.7), contrast=global_contrast)*disk + .5*(1-disk), os.path.join(mc.figpath, name_))\n",
    "mc.in_show_video(name_, figpath=mc.figpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptually, it seems features would take a bit more time to generate, such that the initial direction of an eye that would be smoothly track the motion would first go on the top left and then upwards... \n",
    "\n",
    "## some book keeping for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T16:19:23.177738Z",
     "start_time": "2018-11-07T16:19:23.125993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.7.1 64bit [Clang 10.0.0 (clang-1000.11.45.5)]"
        },
        {
         "module": "IPython",
         "version": "7.1.1"
        },
        {
         "module": "OS",
         "version": "Darwin 18.0.0 x86_64 i386 64bit"
        },
        {
         "module": "numpy",
         "version": "1.15.4"
        },
        {
         "module": "matplotlib",
         "version": "3.0.2"
        },
        {
         "module": "MotionClouds",
         "version": "20180606"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.7.1 64bit [Clang 10.0.0 (clang-1000.11.45.5)]</td></tr><tr><td>IPython</td><td>7.1.1</td></tr><tr><td>OS</td><td>Darwin 18.0.0 x86_64 i386 64bit</td></tr><tr><td>numpy</td><td>1.15.4</td></tr><tr><td>matplotlib</td><td>3.0.2</td></tr><tr><td>MotionClouds</td><td>20180606</td></tr><tr><td colspan='2'>Thu Nov 22 15:48:49 2018 CET</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.7.1 64bit [Clang 10.0.0 (clang-1000.11.45.5)] \\\\ \\hline\n",
       "IPython & 7.1.1 \\\\ \\hline\n",
       "OS & Darwin 18.0.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "numpy & 1.15.4 \\\\ \\hline\n",
       "matplotlib & 3.0.2 \\\\ \\hline\n",
       "MotionClouds & 20180606 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Thu Nov 22 15:48:49 2018 CET} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.7.1 64bit [Clang 10.0.0 (clang-1000.11.45.5)]\n",
       "IPython 7.1.1\n",
       "OS Darwin 18.0.0 x86_64 i386 64bit\n",
       "numpy 1.15.4\n",
       "matplotlib 3.0.2\n",
       "MotionClouds 20180606\n",
       "Thu Nov 22 15:48:49 2018 CET"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, matplotlib, MotionClouds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm -fr {mc.figpath}/*_overlay_*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -l {mc.figpath}/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nikola": {
   "category": "",
   "date": "2018-11-29 11:43:46 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "2018-11-29-feature-vs-global-motion",
   "tags": "motionclouds, code, trajectory",
   "title": "Feature vs global motion",
   "type": "text"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
