{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to compare methods which fit data with psychometric curves using logistic regression. Indeed, after (long) experiments where for instance you collected sequences of keypresses, it is important to infer at best the parameters of the underlying processes: was the observer biased, was she more precise? While I was *forevever* using [sklearn](https://scikit-learn.org/stable/index.html) and praised it's beautifully crafted methods, I lacked some flexibility in the definition of the model. This notebook was done in collaboration with [Jenna Fradin](https://github.com/jennafradin), master student in the lab.\n",
    "\n",
    "Here, I define a similar fitting method using [pytorch](https://pytorch.org/) which fits in a few lines of code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, bias=True, logit0=-3): \n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=bias)    \n",
    "        self.logit0 = torch.nn.Parameter(logit0*torch.ones(1))\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        p0 = self.logit0.sigmoid()\n",
    "        out = p0/2 + (1-p0)*(self.linear(theta).sigmoid())\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.02\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 512\n",
    "#num_epochs = 128 # DEBUG\n",
    "batch_size = 8\n",
    "\n",
    "def fit_data(theta, y, \n",
    "                learning_rate=learning_rate,\n",
    "                num_epochs=num_epochs,\n",
    "                batch_size=batch_size, betas=betas,\n",
    "                verbose=False):\n",
    "\n",
    "    logistic_model = LogisticRegressionModel()\n",
    "\n",
    "    Theta, labels = torch.Tensor(theta[:, None]), torch.Tensor(y[:, None])\n",
    "\n",
    "    loader = DataLoader(TensorDataset(Theta, labels), batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate/len(loader), betas=betas)\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        losses = []\n",
    "        for Theta_, labels_ in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = logistic_model(Theta_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch % (num_epochs//32) == 0) : \n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.5f}\")\n",
    "\n",
    "    logistic_model.eval()\n",
    "    Theta, labels = torch.Tensor(theta[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(Theta)\n",
    "    loss = criterion(outputs, labels).item()/len(theta)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run a series of tests to compare both methods.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "#print(rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "rcParams['font.size'] = fontsize\n",
    "rcParams['legend.fontsize'] = fontsize\n",
    "rcParams['axes.labelsize'] = fontsize\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyper parameters which we will tune later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200            \n",
    "batch_size = N//4\n",
    "#batch_size = N//2\n",
    "\n",
    "N_cv = 10\n",
    "# N_cv = 2\n",
    "\n",
    "seed = 42\n",
    "N_scan = 9\n",
    "N_test = 2000 # number of points for validation\n",
    "\n",
    "bias = True\n",
    "\n",
    "p0 = 0.02         \n",
    "theta0 = 0.\n",
    "wt = np.pi/20      \n",
    "theta_std = np.pi/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem statement: a 2aFC task on synthetic data\n",
    "\n",
    "We will generate a typical setup where we have to guess for the otientation of a visual display compared to the vertical and ask observer to either press on the `left` or `right` arrows. The visual display will be controlled by a $theta$ parameter which we draw randomly according to a Gaussian probability density function. This may be synthesized in the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psychometric_function(theta,\n",
    "                            p0 = p0,\n",
    "                            theta0 = theta0,\n",
    "                            wt = wt,\n",
    "                            ):\n",
    "    return p0/2 + (1-p0) / (1+np.exp(-(theta-theta0)/wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "            N = N,\n",
    "            p0 = p0,\n",
    "            theta0 = theta0,\n",
    "            wt = wt,\n",
    "            theta_std = theta_std,\n",
    "            seed=seed):    \n",
    "    np.random.seed(seed)\n",
    "    theta = np.random.randn(N)*theta_std\n",
    "    \n",
    "    p = psychometric_function(theta, p0, theta0, wt)\n",
    "\n",
    "    y = np.random.rand(N) < p #generate data\n",
    "    return theta, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "211 µs ± 93.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "theta, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Loss: 0.65580\n",
      "Iteration: 16 - Loss: 0.59306\n",
      "Iteration: 32 - Loss: 0.54721\n",
      "Iteration: 48 - Loss: 0.51284\n",
      "Iteration: 64 - Loss: 0.48632\n",
      "Iteration: 80 - Loss: 0.46525\n",
      "Iteration: 96 - Loss: 0.44787\n",
      "Iteration: 112 - Loss: 0.43315\n",
      "Iteration: 128 - Loss: 0.42058\n",
      "Iteration: 144 - Loss: 0.40961\n",
      "Iteration: 160 - Loss: 0.40015\n",
      "Iteration: 176 - Loss: 0.39187\n",
      "Iteration: 192 - Loss: 0.38467\n",
      "Iteration: 208 - Loss: 0.37840\n",
      "Iteration: 224 - Loss: 0.37299\n",
      "Iteration: 240 - Loss: 0.36821\n",
      "Iteration: 256 - Loss: 0.36419\n",
      "Iteration: 272 - Loss: 0.36067\n",
      "Iteration: 288 - Loss: 0.35761\n",
      "Iteration: 304 - Loss: 0.35497\n",
      "Iteration: 320 - Loss: 0.35269\n",
      "Iteration: 336 - Loss: 0.35074\n",
      "Iteration: 352 - Loss: 0.34907\n",
      "Iteration: 368 - Loss: 0.34765\n",
      "Iteration: 384 - Loss: 0.34645\n",
      "Iteration: 400 - Loss: 0.34537\n",
      "Iteration: 416 - Loss: 0.34447\n",
      "Iteration: 432 - Loss: 0.34369\n",
      "Iteration: 448 - Loss: 0.34301\n",
      "Iteration: 464 - Loss: 0.34242\n",
      "Iteration: 480 - Loss: 0.34193\n",
      "Iteration: 496 - Loss: 0.34149\n",
      "Final loss = 0.3410759062834651\n"
     ]
    }
   ],
   "source": [
    "theta, p, y = get_data()\n",
    "logistic_model, loss = fit_data(theta, y,verbose=True)\n",
    "print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 = 0.020, theta0 = 0.000, wt = 0.157, theta_std = 0.524\n"
     ]
    }
   ],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 = 0.006\n",
      "slope = 0.168\n",
      "p0 = 0.002\n"
     ]
    }
   ],
   "source": [
    "def get_params(logistic_model, verbose=False):\n",
    "    theta0_ = -logistic_model.linear.bias.item()/logistic_model.linear.weight.item()\n",
    "    wt_ = 1/logistic_model.linear.weight.item()\n",
    "    p0_ = torch.sigmoid(logistic_model.logit0).item()\n",
    "    if verbose:\n",
    "        if bias: print(f'theta0 = {theta0_:.3f}' )\n",
    "        print(f'slope = {wt_:.3f}')    \n",
    "        print(f'p0 = {p0_:.3f}')        \n",
    "    return theta0_, wt_, p0_\n",
    "\n",
    "theta0_, wt_, p0_ = get_params(logistic_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is fairly quick, in under 2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 s ± 3.81 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do the same thing with `sklearn`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "tol = 1.e-4\n",
    "C = 3.0\n",
    "def fit_data_sklearn(theta, y, \n",
    "                num_epochs=num_epochs,\n",
    "                tol=tol, C=C, \n",
    "                verbose=False):\n",
    "    logistic_model = LogisticRegression(solver='liblinear', max_iter=num_epochs, C=C, tol=tol, fit_intercept=True)\n",
    "    logistic_model.fit(theta[:, None], y)\n",
    "    \n",
    "    outputs = logistic_model.predict_proba(theta[:, None])[:, 1]\n",
    "    outputs_, labels = torch.Tensor(outputs[:, None]), torch.Tensor(y[:, None])\n",
    "    loss = criterion(outputs_, labels).item()/len(theta)\n",
    "    if verbose: print(\"Loss =\", loss)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.34461827952674307\n"
     ]
    }
   ],
   "source": [
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 = 0.006\n",
      "slope = 0.183\n"
     ]
    }
   ],
   "source": [
    "def get_params_sk(logistic_model, verbose=False):\n",
    "    \n",
    "    theta0_ = -logistic_model.intercept_[0]/logistic_model.coef_[0][0]\n",
    "    wt_ = 1/logistic_model.coef_[0][0]\n",
    "\n",
    "    if verbose:\n",
    "        if bias: print(f'theta0 = {theta0_:.3f}' )\n",
    "        print(f'slope = {wt_:.3f}')    \n",
    "    return theta0_, wt_\n",
    "\n",
    "theta0_, wt_ = get_params_sk(logistic_model_sk, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is *much* quicker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 ms ± 412 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but what is the value of few seconds after hours of having observers sitting in front of a screen looking at (often boring) visual displays? More seriously, most important is the reliability of the values which are inferred by each respective method, such that they are correctly reflecting the information contained in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qualitative comparison of methods\n",
    "\n",
    "We can synthesize this comparison by drawing a new dataset and plotting the psychometric curves which are obtained by each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss = 0.342\n",
      "Training sklearn loss = 0.345\n"
     ]
    }
   ],
   "source": [
    "theta, p, y = get_data() # nouvelles données \n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f'Training loss = {loss:.3f}')\n",
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f'Training sklearn loss = {loss_sk:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100, 1) and (200,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c43756367c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_values_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsychometric_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_values_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hidden proba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100, 1) and (200,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(theta, y, s=6, alpha=.4, color = 'k', label='data points')\n",
    "# ax.scatter(theta, p, s=6, alpha=.4, color = 'b', label='hidden proba')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values_p = psychometric_function(theta, p0, theta0, wt)\n",
    "ax.plot(x_values, y_values_p, alpha=.4, color = 'b', label='hidden proba')\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "ax.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='torch')\n",
    "y_values_sk = logistic_model_sk.predict_proba(x_values)[:, 1]\n",
    "ax.plot(x_values, y_values_sk, 'r', alpha=.7, lw=3, label='sklearn')\n",
    "ax.set_xlabel(r'orientation $\\theta$', fontsize=20)\n",
    "ax.set_yticks([0.,1.])\n",
    "ax.set_yticklabels(['Left', 'Right'], fontsize=20)\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses which were computed above are those obtained during training. Relying on this value may be a dangerous strategy as the model may be overfitting the data. We should therefore measure how the model would generalize with novel data.\n",
    "\n",
    "While it hard to do with real (experimental) data which are often scarse, here we synthesized the data and we can thus compute a testing loss by drawing again a set of new data and computing the loss on that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data(N=N_test, seed=seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_true(theta, p, y):\n",
    "    # p0=p0, theta0=theta0, wt=wt, \n",
    "    # p = p0/2 + (1-p0) / (1+np.exp(-(theta-theta0)/wt))\n",
    "\n",
    "    labels = torch.Tensor(y[:, None])        \n",
    "    P = torch.Tensor(p[:, None])\n",
    "    return criterion(P, labels).item()/len(theta)\n",
    "print(f'Testing true loss = {loss_true(theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f'Training loss = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_torch(logistic_model, theta, p, y):\n",
    "    theta, p, y = get_data(N=N_test, seed=seed)\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    Theta = torch.Tensor(theta[:, None])\n",
    "    P = torch.Tensor(p[:, None])\n",
    "\n",
    "    outputs = logistic_model(Theta)\n",
    "    return criterion(outputs, labels).item()/len(theta)\n",
    "print(f'Testing loss = {loss_torch(logistic_model, theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f'Training sklearn loss = {loss_sk:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_sklearn(logistic_model, theta, p, y):\n",
    "    theta, p, y = get_data(N=N_test, seed=seed)\n",
    "    \n",
    "    outputs = logistic_model.predict_proba(theta[:, None])[:, 1]\n",
    "    outputs_, labels = torch.Tensor(outputs[:, None]), torch.Tensor(y[:, None])\n",
    "    return criterion(outputs_, labels).item()/len(theta)\n",
    "\n",
    "print(f'Testing sklearn loss = {loss_sklearn(logistic_model_sk, theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## quantitative comparison of methods : varrying methods' parameters\n",
    "\n",
    "Let's study the influence of each method's meta-parameter, such as the number of iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learning_rate * np.logspace(-1, 1, N_scan, base=10)\n",
    "learning_rates_, losses, loss_Ps = [], [], []\n",
    "for learning_rate_ in learning_rates:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, learning_rate=learning_rate_, verbose=False)        \n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"learning_rate: {learning_rate_:.5f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        learning_rates_.append(learning_rate_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(marker='.', lw=0, alpha=3/N_cv, ms=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#influence du learning rate sur loss\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8))\n",
    "ax.plot(learning_rates_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(learning_rates_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlim(np.min(learning_rates_), np.max(learning_rates_))\n",
    "\n",
    "ax.set_xlabel('learning_rate')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence du nombre d'epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochss = num_epochs * np.logspace(-1, 1, N_scan, base=10)\n",
    "num_epochss_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "for num_epochs_ in num_epochss:\n",
    "    for i_CV in range(N_cv):\n",
    "        \n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "                \n",
    "        if i_CV==0: \n",
    "            print(f\"num_epochs: {int(num_epochs_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        num_epochss_.append(num_epochs_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence du nbr d'epochs sur loss \n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(num_epochss_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(num_epochss_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(num_epochss_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('# epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of minibatch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = N * np.logspace(-3, 0, N_scan, base=2)\n",
    "batch_sizes_, losses, loss_Ps = [], [], []\n",
    "for batch_size_ in batch_sizes:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, batch_size=int(batch_size_), verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"batch_size: {int(batch_size_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        batch_sizes_.append(batch_size_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence de la taille du minibatch sur loss \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(batch_sizes_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(batch_sizes_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('batch_size')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1s = 1. - np.logspace(-3, -1, N_scan, base=10, endpoint=True)\n",
    "beta1s_, losses, loss_Ps = [], [], []\n",
    "for beta1_ in beta1s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, betas=(beta1_, beta2), verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"beta1: {beta1_}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        beta1s_.append(beta1_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(beta1s_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(beta1s_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('beta1')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2s = 1. - np.logspace(-3, -1, N_scan, base=10, endpoint=True)\n",
    "beta2s_, losses, loss_Ps = [], [], []\n",
    "for beta2_ in beta2s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, betas=(beta1, beta2_), verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"beta2: {beta2_}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        beta2s_.append(beta2_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(beta2s_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(beta2s_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('beta1')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = C * np.logspace(-2, 2, N_scan, base=4)\n",
    "Cs_, loss_Ps, loss_SKLs = [], [], []\n",
    "for C_ in Cs:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, C=C_, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"C: {C_}, Loss: {loss:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "            \n",
    "        Cs_.append(C_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(Cs_, loss_Ps, **opts, color='blue', label='true')\n",
    "ax.plot(Cs_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `tol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = tol * np.logspace(-2, 2, N_scan, base=10)\n",
    "tols_, loss_Ps, loss_SKLs = [], [], []\n",
    "for tol_ in tols:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, tol=tol_, verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"tol: {tol_}, Loss: {loss:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        tols_.append(tol_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(tols_, loss_Ps, **opts, color='blue', label='true')\n",
    "ax.plot(tols_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('tol')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## quantitative comparison of methods : varrying experimental parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = np.logspace(1, 3, N_scan, base=10, endpoint=True)\n",
    "\n",
    "Ns_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for N_ in Ns:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: print(f\"N: {int(N_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        Ns_.append(N_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(Ns_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(Ns_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(Ns_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of number of theta_std\n",
    "\n",
    "\n",
    "The convergence of the fitting procedure may also depend on the parametrers of the data which were set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_stds = theta_std * np.logspace(-1, 1, N_scan, base=2, endpoint=True)\n",
    "\n",
    "theta_stds_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for theta_std_ in theta_stds:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(theta_std=theta_std_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(theta_std=theta_std_, N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: print(f\"theta_std: {theta_std_:.3f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        theta_stds_.append(theta_std_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(theta_stds_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(theta_stds_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(theta_stds_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of number of `p0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0s = np.logspace(-3, -.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for p0_ in p0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(p0=p0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(p0=p0_, N=N_test, seed=seed+i_CV)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: print(f\"theta_std: {theta_std_:.3f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        p0s_.append(p0_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(theta_stds_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(theta_stds_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(theta_stds_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## comparing the predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `p0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "p0s = np.logspace(-3, -.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for p0_ in p0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(p0=p0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0_)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(p0s_, p0_tos, label='torch')\n",
    "axs[0].plot([min(p0s_), max(p0s_)], [min(p0_tos), max(p0_tos)], '--')\n",
    "axs[0].set(xlabel='p0 (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(theta0s_, theta0_tos, label='torch')\n",
    "axs[1].scatter(theta0s_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(theta0s_), max(theta0s_)], [min(theta0_sks), max(theta0_sks)], '--')\n",
    "axs[1].set(xlabel='theta0 (true)', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label='torch')\n",
    "axs[2].scatter(wts_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], '--')\n",
    "axs[2].set(xlabel='slope (true)', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `theta0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "theta0s = theta_std * np.linspace(-1, 1, N_scan, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for theta0_ in theta0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(theta0=theta0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0_)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(p0s_, p0_tos, label='torch')\n",
    "axs[0].plot([min(p0s_), max(p0s_)], [min(p0_tos), max(p0_tos)], '--')\n",
    "axs[0].set(xlabel='p0 (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(theta0s_, theta0_tos, label='torch')\n",
    "axs[1].scatter(theta0s_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(theta0s_), max(theta0s_)], [min(theta0_sks), max(theta0_sks)], '--')\n",
    "axs[1].set(xlabel='theta0 (true)', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label='torch')\n",
    "axs[2].scatter(wts_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], '--')\n",
    "axs[2].set(xlabel='slope (true)', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `wt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "wts = wt * np.logspace(-1, 1, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for wt_ in wts:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(wt=wt_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt_)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(p0s_, p0_tos, label='torch')\n",
    "axs[0].plot([min(p0s_), max(p0s_)], [min(p0_tos), max(p0_tos)], '--')\n",
    "axs[0].set(xlabel='p0 (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(theta0s_, theta0_tos, label='torch')\n",
    "axs[1].scatter(theta0s_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(theta0s_), max(theta0s_)], [min(theta0_sks), max(theta0_sks)], '--')\n",
    "axs[1].set(xlabel='theta0 (true)', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label='torch')\n",
    "axs[2].scatter(wts_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], '--')\n",
    "axs[2].set(xlabel='slope (true)', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.22.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
