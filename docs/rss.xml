<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook</title><link>https://laurentperrinet.github.io/sciblog/</link><description>

This is my scientific logbook.
Experiments happen here in an apparently random order,
most of the time with more questions than answers...
Do not hesitate to comment!
</description><atom:link href="https://laurentperrinet.github.io/sciblog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Wed, 08 Dec 2021 19:46:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Logistic regression</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-12-12-logistic-regression.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In a previous notebook, I have shown how to &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-04-08-fitting-a-psychometric-curve-using-pytorch.html"&gt;fit a psychometric curve using pyTorch&lt;/a&gt;. Here, I would like to review some nice properies of the logistic regression model (WORK IN PROGRESS).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-12-12-logistic-regression.html"&gt;Read more…&lt;/a&gt; (4 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>computational neuroscience</category><category>deep-learning</category><category>machine-learning</category><category>math</category><category>python</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-12-12-logistic-regression.html</guid><pubDate>Sun, 12 Dec 2021 09:32:39 GMT</pubDate></item><item><title>Experimenting with transfer learning for visual categorization</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-04-28-experimenting-with-transfer-learning-for-visual-categorization.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Hi! I am  &lt;a href="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/"&gt;Jean-Nicolas Jérémie&lt;/a&gt; and the goal of this notebook is to provide a framework to implement (and experiment with) &lt;a href="https://en.wikipedia.org/wiki/Transfer_learning"&gt;transfer learning&lt;/a&gt; on deep convolutional neuronal network (DCNN). In a nutshell, &lt;a href="https://www.analyticsvidhya.com/blog/2021/06/transfer-learning-using-vgg16-in-pytorch/"&gt;transfer learning&lt;/a&gt; allows to re-use the knowlegde learned on a problem, such as categorizing images from  a large dataset, and apply it to a different (yet related) problem, performing the categorization on a smaller dataset. It is a powerful method as it allows to implement complex task &lt;em&gt;de novo&lt;/em&gt; quite rapidly (in a few hours) without having to retrain the millions of parameters of a DCNN (which takes days of computations). The basic hypothesis is that it suffices to &lt;a href="https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch"&gt;re-train the last classification layers&lt;/a&gt; (the head) while keeping the first layers fixed. Here, these networks teach us also some interesting insights into how living systems may perform such categorization tasks.&lt;/p&gt;
&lt;p&gt;Based on our &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html"&gt;previous work&lt;/a&gt;, we will start from a &lt;a href="https://pytorch.org/hub/pytorch_vision_vgg/"&gt;VGG16 network&lt;/a&gt; loaded from the &lt;a href="https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py"&gt;&lt;code&gt;torchvision.models&lt;/code&gt; library&lt;/a&gt; and pre-trained on the &lt;a href="http://image-net.org/"&gt;Imagenet&lt;/a&gt; dataset wich allows to perform label detection on naturals images for $K = 1000$ labels. Our goal here will be to re-train the last fully-Connected layer of the network to perfom the same task but in a sub-set of $K = 10$ labels from the Imagenet dataset.&lt;/p&gt;
&lt;p&gt;Moreover, we are going to evaluate different strategies of transfer learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VGG General : Substitute the last layer of the pyTorch VGG16 network ($K = 1000$ labels) with a new layer build from a specific subset ($K = 10$ labels).&lt;/li&gt;
&lt;li&gt;VGG Linear : Add a new layer build from a specific subset ($K = 10$ labels) after the last Fully-Connected layer of the the pyTorch VGG16 network.&lt;/li&gt;
&lt;li&gt;VGG Gray : Same architecture as the VGG General network but trained with grayscale images.&lt;/li&gt;
&lt;li&gt;VGG Scale : Same architecture as the VGG General network but trained with images of different size.&lt;/li&gt;
&lt;li&gt;VGG Full : Same architecture as the VGG General network but all the layers are trained (otherwise I trained the last Fully-Connected layer). &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this notebook, I will use the &lt;a href="https://pytorch.org/"&gt;pyTorch&lt;/a&gt; library for running the networks and the &lt;a href="https://pandas.pydata.org/docs/getting_started/index.html"&gt;pandas&lt;/a&gt; library to collect and display the results. This notebook was done during a master 2 internship at the Neurosciences Institute of Timone (INT) under the supervision of &lt;a href="https://laurentperrinet.github.io/"&gt;Laurent Perrinet&lt;/a&gt;. It is curated in the following &lt;a href="https://github.com/JNJER/2021-04-28_transfer_learning"&gt;github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-04-28-experimenting-with-transfer-learning-for-visual-categorization.html"&gt;Read more…&lt;/a&gt; (88 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>computational neuroscience</category><category>deep-learning</category><category>jupyter</category><category>learning</category><category>machine-learning</category><category>open-science</category><category>pandas</category><category>python</category><category>pytorch</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-04-28-experimenting-with-transfer-learning-for-visual-categorization.html</guid><pubDate>Fri, 03 Dec 2021 15:23:51 GMT</pubDate></item><item><title>Find a 8 in a forest of 9</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-12-01-find-a-8-in-a-forest-of-9.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to find one non-repeting pattern in an image. First, find the pattern, then second, compute correlation.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-12-01-find-a-8-in-a-forest-of-9.html"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>grand-public</category><category>math</category><category>outreach</category><category>python</category><category>space</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-12-01-find-a-8-in-a-forest-of-9.html</guid><pubDate>Wed, 01 Dec 2021 20:30:16 GMT</pubDate></item><item><title>Triangulating stars on the night sky</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-12-01-triangulating-stars-on-the-night-sky.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In a previous notebook, I have shown properties of the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html"&gt;distribution of stars in the sky&lt;/a&gt;. Here, I would like to use the existing database of stars' positions and display them as a triangulation&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-12-01-triangulating-stars-on-the-night-sky.html"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>grand-public</category><category>outreach</category><category>python</category><category>space</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-12-01-triangulating-stars-on-the-night-sky.html</guid><pubDate>Wed, 01 Dec 2021 09:32:39 GMT</pubDate></item><item><title>Modelling Ocean surface waves</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-09-11-modelling-ocean-surface-waves.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Trying to model Ocean surface waves, I have played using a &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2014-10-24_a-bit-of-fun-with-gravity-waves.html"&gt;linear superposition of sinusoids&lt;/a&gt; and also on the fact that &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2016-04-24_a-wave-going-backwards.html"&gt;phase speed (following a wave's crest) is twice as fast as group speed (following a group of waves)&lt;/a&gt;. Finally, I more recently used such generation of a model of the random superposition of waves to model the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-06-19-caustic-optics.html"&gt;wigggling lines formed by the refraction (bendings of the trajectory of a ray at the interface between air and water) of light, called caustics&lt;/a&gt;...&lt;/p&gt;
&lt;p&gt;Observing real-life ocean waves instructed me that if a single wave is well approximated by a sinusoïd, each wave is qualitatively a bit different. Typical for these surface gravity waves are &lt;a href="https://en.wikipedia.org/wiki/Stokes_wave"&gt;the sharp crests and flat troughs&lt;/a&gt;. As a matter of fact, modelling ocean waves is on one side very useful (&lt;a href="https://en.wikipedia.org/wiki/Ocean_dynamics"&gt;Ocean dynamics&lt;/a&gt; and its impact on climate, modelling tides, tsunamis, diffraction in a bay to predict coastline evolution, ...) but quite demanding despite a &lt;a href="https://en.wikipedia.org/wiki/Airy_wave_theory"&gt;well known mathematical model&lt;/a&gt;. Starting with the Navier-Stokes equations to an incompressible fluid (water) in a gravitational field leads to &lt;a href="https://en.wikipedia.org/wiki/Luke%27s_variational_principle"&gt;Luke's variational principle&lt;/a&gt; in certain simplifying conditions. Further simplifications lead to the approximate solution given by Stokes which gives the following shape as the sum of different harmonics:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Stokes3_wave.svg/2880px-Stokes3_wave.svg.png" alt="Stokes wave(https://en.wikipedia.org/wiki/Stokes_wave)"&gt;&lt;/p&gt;
&lt;p&gt;This seems well enough at the moment and I will capture this shape in this notebook and notably if that applies to a random mixture of such waves...&lt;/p&gt;
&lt;p&gt;(WORK IN PROGRESS)&lt;/p&gt;
&lt;p&gt;The current situation is that these solutions seem to not fit what is displayed on the wikipedia page and that I do not spot the bug I may have introduced... More work on this is needed and any feedback is welcome...&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-09-11-modelling-ocean-surface-waves.html"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>elasticite</category><category>math</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-09-11-modelling-ocean-surface-waves.html</guid><pubDate>Sat, 11 Sep 2021 09:56:27 GMT</pubDate></item><item><title>Density of stars on the surface of the sky</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/191391751"&gt;&lt;img src="https://zenodo.org/badge/191391751.svg" alt="DOI"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Looking at the night sky, the pattern of stars on the surface of the sky follows a familiar pattern. The Big Dipper, Cassiopeia, the Pleiades, or Orion are popular landmarks in the sky which we can immediatly recognize.&lt;/p&gt;
&lt;p&gt;Different civilisations labelled these patterns using names such as constellations in the western world. However, this pattern is often the result of pure chance. Stars from one constellation belong often to remote areas in the universe and they bear this familiarity only because we always saw them as such; the rate at which stars move is much shorter than the lifespan of humanity.&lt;/p&gt;
&lt;p&gt;I am curious here to study the density of stars as they appear on the surface of the sky. It is both a simple question yet a complex to formulate. Is there any generic principle that could be used to characterize their distribution? This is my attempt to answer the question&lt;/p&gt;
&lt;p&gt;&lt;a href="https://astronomy.stackexchange.com/questions/43147/density-of-stars-on-the-surface-of-the-sky"&gt;https://astronomy.stackexchange.com/questions/43147/density-of-stars-on-the-surface-of-the-sky&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(but also to make the formulation of the question clearer...). This is my answer:
&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html"&gt;Read more…&lt;/a&gt; (33 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>grand-public</category><category>math</category><category>moviepy</category><category>outreach</category><category>python</category><category>space</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html</guid><pubDate>Sat, 27 Mar 2021 13:06:52 GMT</pubDate></item><item><title>Time lapsing an orchid's flower</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to realize a time-lapse using a raspberry π and some python code and finally get this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/laurentperrinet/TimeTeleScope/raw/main/videos/2021-02-17_TimeTeleScope.gif" alt=""&gt;&lt;/p&gt;
&lt;p&gt;(This was done over 10 days, with &lt;em&gt;almost&lt;/em&gt; each day an &lt;em&gt;irregular&lt;/em&gt; session the morning and one in the evening)&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>grand-public</category><category>learning</category><category>moviepy</category><category>numpy</category><category>outreach</category><category>python</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html</guid><pubDate>Sun, 21 Feb 2021 19:57:52 GMT</pubDate></item><item><title>Kuramoto model</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to re-implement the &lt;a href="https://en.wikipedia.org/wiki/Kuramoto_model"&gt;Kuramoto model&lt;/a&gt;, following a lecture from &lt;a href="https://sites.google.com/site/cvjoanacabral/"&gt;Joana Cabral&lt;/a&gt; and the &lt;a href="https://drive.google.com/drive/folders/1UGee2uB9y0FLbftxIRx-89KFeA87RifC"&gt;code&lt;/a&gt; that is provided, but using python instead of matlab.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>computational neuroscience</category><category>learning</category><category>math</category><category>neural</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html</guid><pubDate>Fri, 08 Jan 2021 13:42:21 GMT</pubDate></item><item><title>Generating second-order figures from texture</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to use the &lt;a href="https://github.com/NeuralEnsemble/MotionClouds"&gt;MotionClouds&lt;/a&gt; library to generate figures with second-order contours, similar to those used in the &lt;a href="https://nin.nl/research/researchgroups/roelfsema-group/"&gt;P. Roelfsema's group&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>motionclouds</category><category>orientation</category><category>python</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html</guid><pubDate>Fri, 08 Jan 2021 10:54:45 GMT</pubDate></item><item><title>Extracting music from the screenshots of a Spotify playlist</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post I'll try to show how from a screenshot obtained from a software like Spotify you can programmatically extract the tracks of the songs as well as the artists, to finally download them from the Internet.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html"&gt;Read more…&lt;/a&gt; (134 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>machine-learning</category><category>open-science</category><category>pandas</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html</guid><pubDate>Tue, 24 Nov 2020 14:19:07 GMT</pubDate></item></channel></rss>