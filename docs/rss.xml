<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook</title><link>https://laurentperrinet.github.io/sciblog/</link><description>

This is my scientific logbook.
Experiments happen here in an apparently random order,
most of the time with more questions than answers...
Do not hesitate to comment!
</description><atom:link href="https://laurentperrinet.github.io/sciblog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Wed, 05 May 2021 08:49:24 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>2021-03-27 Density of stars on the surface of the sky</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Looking at the night sky, the pattern of stars on the surface of the sky follows a familiar pattern. The Big Dipper, Cassiopeia, the Pleiades, or Orion are popular landmarks in the sky which we can immediatly recognize.&lt;/p&gt;
&lt;p&gt;Different civilisations labelled these patterns using names such as constellations in the western world. However, this pattern is often the result of pure chance. Stars from one constellation belong often to remote areas in the universe and they bear this familiarity only because we always saw them as such; the rate at which stars move is much shorter than the lifespan of humanity.&lt;/p&gt;
&lt;p&gt;I am curious here to study the density of stars as they appear on the surface of the sky. It is both a simple question yet a complex to formulate. Is there any generic principle that could be used to characterize their distribution? This is my attempt to answer the question&lt;/p&gt;
&lt;p&gt;&lt;a href="https://astronomy.stackexchange.com/questions/43147/density-of-stars-on-the-surface-of-the-sky"&gt;https://astronomy.stackexchange.com/questions/43147/density-of-stars-on-the-surface-of-the-sky&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(but also to make the formulation of the question clearer...). This is my answer:
&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html"&gt;Read more…&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>grand-public</category><category>moviepy</category><category>outreach</category><category>python</category><category>space</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-03-27-density-of-stars-on-the-surface-of-the-sky.html</guid><pubDate>Sat, 27 Mar 2021 13:06:52 GMT</pubDate></item><item><title>Time lapsing an orchid's flower</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to realize a time-lapse using a raspberry π and some python code and finally get this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/laurentperrinet/TimeTeleScope/raw/main/videos/2021-02-17_TimeTeleScope.gif" alt=""&gt;&lt;/p&gt;
&lt;p&gt;(This was done over 10 days, with &lt;em&gt;almost&lt;/em&gt; each day an &lt;em&gt;irregular&lt;/em&gt; session the morning and one in the evening)&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>grand-public</category><category>learning</category><category>moviepy</category><category>numpy</category><category>outreach</category><category>python</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-02-21-time-lapsing-an-orchids-flower.html</guid><pubDate>Sun, 21 Feb 2021 19:57:52 GMT</pubDate></item><item><title>Kuramoto model</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to re-implement the &lt;a href="https://en.wikipedia.org/wiki/Kuramoto_model"&gt;Kuramoto model&lt;/a&gt;, following a lecture from &lt;a href="https://sites.google.com/site/cvjoanacabral/"&gt;Joana Cabral&lt;/a&gt; and the &lt;a href="https://drive.google.com/drive/folders/1UGee2uB9y0FLbftxIRx-89KFeA87RifC"&gt;code&lt;/a&gt; that is provided, but using python instead of matlab.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>learning</category><category>neural</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-kuramoto-model.html</guid><pubDate>Fri, 08 Jan 2021 13:42:21 GMT</pubDate></item><item><title>Generating second-order figures from texture</title><link>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The goal here is to use the &lt;a href="https://github.com/NeuralEnsemble/MotionClouds"&gt;MotionClouds&lt;/a&gt; library to generate figures with second-order contours, similar to those used in the &lt;a href="https://nin.nl/research/researchgroups/roelfsema-group/"&gt;P. Roelfsema's group&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>motionclouds</category><category>orientation</category><category>python</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2021-01-08-generating-second-order-figures-from-texture.html</guid><pubDate>Fri, 08 Jan 2021 10:54:45 GMT</pubDate></item><item><title>Extracting music from the screenshots of a Spotify playlist</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;In this post I'll try to show how from a screenshot obtained from a software like Spotify you can programmatically extract the tracks of the songs as well as the artists, to finally download them from the Internet.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html"&gt;Read more…&lt;/a&gt; (134 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>open-science</category><category>pandas</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-11-24-extracting-music-from-the-screenshots-of-a-spotify-playlist.html</guid><pubDate>Tue, 24 Nov 2020 14:19:07 GMT</pubDate></item><item><title>Fitting COVID data</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-10-10-fitting-covid-data.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I propose here a simple method to fit experimental data common to epidemiological spreads, such as the present COVID-19 pandemic, using the &lt;a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution"&gt;inverse gaussian distribution&lt;/a&gt;. This follows the general incompregension of my answer to the question &lt;a href="https://stats.stackexchange.com/questions/455202/is-the-covid-19-pandemic-curve-a-gaussian-curve/456016#456016"&gt;Is the COVID-19 pandemic curve a Gaussian curve?&lt;/a&gt; on StackOverflow. My initial point is to say that a Gaussian is not adapted as it handles a distribution on real numbers, while such a curve (the variable being number of days) handles numbers on the half line. Inspired by the &lt;em&gt;excellent&lt;/em&gt; &lt;a href="https://core.ac.uk/display/141211187"&gt;A Theory of Reaction Time Distributions&lt;/a&gt; by Dr Fermin Moscoso del Prado Martin a constructive approach is to propose another distribution, such as the inverse Gaussian distribution.&lt;/p&gt;
&lt;p&gt;This notebook develops this same idea on real data and proves &lt;em&gt;numerically&lt;/em&gt; how bad the Gaussian fit is compared to the latter. Thinking about the importance of doing a proper inference in such a case, I conclude&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-10-10-fitting-covid-data.html"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>machine-learning</category><category>math</category><category>numpy</category><category>open-science</category><category>pandas</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-10-10-fitting-covid-data.html</guid><pubDate>Sat, 10 Oct 2020 12:13:12 GMT</pubDate></item><item><title>Benchmarking CNNs</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Hi! I am  &lt;a href="https://github.com/JNJER"&gt;Jean-Nicolas Jérémie&lt;/a&gt; and the goal of this benchmark is to offer a comparison between differents pre-trained image recognition's networks based on the &lt;a href="http://image-net.org/"&gt;Imagenet&lt;/a&gt; dataset wich allows to work on naturals images for $1000$ labels. These different networks tested here are taken from the &lt;code&gt;torchvision.models&lt;/code&gt; library : &lt;code&gt;AlexNet&lt;/code&gt;, &lt;code&gt;VGG16&lt;/code&gt;, &lt;code&gt;MobileNetV2&lt;/code&gt; and &lt;code&gt;ResNet101&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Our use case is to measure the performance of a system which receives a sequence of images and has to make a decision as soon as possible, hence with &lt;code&gt;batch_size=1&lt;/code&gt;. Specifically, we wish also to compare different computing architectures such as CPUs, desktop GPUs or other more exotic platform such as the Jetson TX2 (experiment 1). Additionally, we will implement some image transformations as up/down-sampling (experiment 2) or transforming to grayscale (experiment 3) to quantify their influence on the accuracy and computation time of each network.&lt;/p&gt;
&lt;p&gt;In this notebook, I will use the &lt;a href="https://pytorch.org/"&gt;Pytorch&lt;/a&gt; library for running the networks and the &lt;a href="https://pandas.pydata.org/docs/getting_started/index.html"&gt;pandas&lt;/a&gt; library to collect and display the results. This notebook was done during a master 1 internship at the Neurosciences Institute of Timone (INT) under the supervision of &lt;a href="https://laurentperrinet.github.io/"&gt;Laurent PERRINET&lt;/a&gt;. It is curated in the following &lt;a href="https://github.com/JNJER/2020-06-26_fast_and_curious"&gt;github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html"&gt;Read more…&lt;/a&gt; (35 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>deep-learning</category><category>jupyter</category><category>machine-learning</category><category>open-science</category><category>pandas</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-09-28-benchmarking-cnns.html</guid><pubDate>Mon, 28 Sep 2020 15:25:57 GMT</pubDate></item><item><title>nesting jupyter runs</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-08-09-nesting-jupyter-runs.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://jupyter.org/"&gt;Jupyter notebooks&lt;/a&gt; are a great way of sharing knowledge in science, art, programming. 
For instance, in a recent musing, I tried to programmatically determine the &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-07-04-colors-of-the-sky.html"&gt;color of the sky&lt;/a&gt;. This renders as a web page, but is also a piece of &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-07-04-colors-of-the-sky.ipynb"&gt;runnable code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As such, they are also great ways to store the knowledge that was acquired at a given time and that could be reusable. This may be considered as bad programming and may have downsides as described in that &lt;a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1"&gt;slides&lt;/a&gt; :&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1"&gt;&lt;img src="https://revolution-computing.typepad.com/.a/6a010534b1db25970b022ad3b0836c200b-800wi" alt="Joel Grus"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Recently, thanks to an answer to a &lt;a href="https://stackoverflow.com/questions/48067529/ipython-run-magic-n-switch-not-working"&gt;stack overflow question&lt;/a&gt;,  I found a way to overcome this by detecting if the caall to a notebook is made from the notebook itself or from a parent.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-08-09-nesting-jupyter-runs.html"&gt;Read more…&lt;/a&gt; (1861 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>ipython</category><category>jupyter</category><category>open-science</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-08-09-nesting-jupyter-runs.html</guid><pubDate>Sun, 09 Aug 2020 08:52:18 GMT</pubDate></item><item><title>Colors of the sky</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-07-04-colors-of-the-sky.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Our sensorial environment contains multiple regularities which our brain uses to optimize its representation of the world: objects fall most of the time downwards, the nose is usually in the middle below the eyes, the &lt;em&gt;sky is blue&lt;/em&gt;... Concerning this last point, I wish here to illustrate the physical origins of this phenomenon and in particular the range of colors that you may observe in the sky.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-07-04-colors-of-the-sky.html"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>grand-public</category><category>motionclouds</category><category>numpy</category><category>open-science</category><category>outreach</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-07-04-colors-of-the-sky.html</guid><pubDate>Sat, 04 Jul 2020 10:05:58 GMT</pubDate></item><item><title>Caustic (optics)</title><link>https://laurentperrinet.github.io/sciblog/posts/2020-06-19-caustic-optics.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Caustics (&lt;a href="https://en.wikipedia.org/wiki/Caustic_(optics"&gt;wikipedia&lt;/a&gt;) are luminous patterns which are resulting from the superposition of smoothly deviated light rays. It is for instance the heart-shaped pattern in your cup of coffee which is formed as the rays of the sun are reflected on the cup's surface. It is also the wiggly pattern of light curves that you will see on the floor of a pool as the sun's light is &lt;em&gt;refracted&lt;/em&gt; at the surface of the water. Here, we simulate that particular physical phenomenon. Simply because they are mesmerizingly beautiful, but also as it is of interest in visual neuroscience. Indeed, it speaks to how images are formed (more on this later), hence how the brain may understand images.&lt;/p&gt;
&lt;p&gt;In this post, I will develop a simple formalism to generate such patterns, with the paradoxical result that it is &lt;em&gt;very&lt;/em&gt; simple to code yet generates patterns with great complexity, such as:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;center&gt;&lt;img src="https://laurentperrinet.github.io/sciblog/files/2020-06-19_caustique/2020-06-19_caustique.gif" width="61.8%/"&gt; &lt;/center&gt;
&lt;br&gt;

&lt;p&gt;This is joint work with artist &lt;a href="https://laurentperrinet.github.io/authors/etienne-rey/"&gt;Etienne Rey&lt;/a&gt;, in which I especially follow the ideas put forward in the series &lt;a href="http://ondesparalleles.org/projets/turbulences/"&gt;Turbulence&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/273226625"&gt;&lt;img src="https://zenodo.org/badge/273226625.svg" alt="DOI"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2020-06-19-caustic-optics.html"&gt;Read more…&lt;/a&gt; (39 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>elasticite</category><category>grand-public</category><category>motionclouds</category><category>numpy</category><category>open-science</category><category>outreach</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2020-06-19-caustic-optics.html</guid><pubDate>Fri, 19 Jun 2020 08:01:02 GMT</pubDate></item></channel></rss>