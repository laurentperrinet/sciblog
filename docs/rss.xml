<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook</title><link>https://laurentperrinet.github.io/sciblog/</link><description>This is my scientific logbook. Experiments happen here in an apparently random order, most of the time with more questions than answers... Do not hesitate to comment!</description><atom:link href="https://laurentperrinet.github.io/sciblog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Wed, 20 Aug 2025 15:11:27 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Extending the high-phi illusion</title><link>https://laurentperrinet.github.io/sciblog/posts/2025-06-09-extending-the-high-phi-illusion.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The  High Phi Motion illusion, is the illusory perception of a strong shift of motion induced by a slow inducing motion. A demo page is available on the &lt;a href="http://lab-perception.org/demo/highphi/"&gt;min author's webpage&lt;/a&gt; and the effect is described in this excellent paper :
Wexler M, Glennerster A, Cavanagh P, Ito H &amp;amp; Seno T (2013). Default perception of high-speed motion. PNAS, 110, 7080-7085. &lt;a href="http://wexler.free.fr/papers/highphi.pdf"&gt;http://wexler.free.fr/papers/highphi.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this notebook, we will generate an extension of this illusion to answer to the question of knowing if it limited to the one-dimensional motion along the ring or if this can extended to arbitrary, 2D, planar motions. This will help decipher some of the factors leading to this "illusion".&lt;/p&gt;
&lt;p&gt;TL;DR : one can reproduce the illusion on a planar motion (not a rotation), but it seems important that the motion is either limited to a band-like shape or to limited orientations:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [28]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Video&lt;/span&gt;

&lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'2025-06-09_extending-the-high-phi-illusion'&lt;/span&gt;
&lt;span class="n"&gt;Video&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'../files/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;/high-phi.mp4'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;html_attributes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"loop=True autoplay=True  controls=True"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[28]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;&lt;video autoplay="True" controls="True" loop="True" src="https://laurentperrinet.github.io/sciblog/files/2025-06-09_extending-the-high-phi-illusion/high-phi.mp4"&gt;
      Your browser does not support the &lt;code&gt;video&lt;/code&gt; element.
    &lt;/video&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [29]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Video&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;'../files/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;/high-phi-oriented-inducer-noband.mp4'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;html_attributes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"loop=True autoplay=True  controls=True"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[29]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;&lt;video autoplay="True" controls="True" loop="True" src="https://laurentperrinet.github.io/sciblog/files/2025-06-09_extending-the-high-phi-illusion/high-phi-oriented-inducer-noband.mp4"&gt;
      Your browser does not support the &lt;code&gt;video&lt;/code&gt; element.
    &lt;/video&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2025-06-09-extending-the-high-phi-illusion.html"&gt;Read more…&lt;/a&gt; (11 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>behavior</category><category>blog</category><category>motion</category><category>motionclouds</category><category>psychophysics</category><category>python</category><category>retina</category><category>saccades</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2025-06-09-extending-the-high-phi-illusion.html</guid><pubDate>Mon, 09 Jun 2025 07:16:40 GMT</pubDate></item><item><title>Orienting yourself in the visual flow</title><link>https://laurentperrinet.github.io/sciblog/posts/2025-04-24-orienting-yourself-in-the-visual-flow.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Moving through the world depends on our ability to perceive and interpret visual information, with optic flow playing a crucial role. Optic flow provides essential cues for self-motion and navigation, helping us to determine our direction, speed and the structure of our environment. In particular, the radial pattern of motion we see as we move allows to atune our gaze with respect to our self motion. By targeting the focal point of expansion within this flow, we can accurately orient ourselves and adjust our trajectory. This will not only deepen our understanding of visual perception, but also highlight the practical importance of optic flow in everyday navigation, from avoiding obstacles to maintaining stability.&lt;/p&gt;
&lt;p&gt;In this notebook, we will generate a synthetic optic flow to challenge and examine how the visual system captures the focus of expansion. For this we will create a movie allowing to create an optic flow corresponding to moving inside a textured tunnel, potentially moving the center of the focus of expansion with respect to the center of gaze.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.display&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Video&lt;/span&gt;

&lt;span class="n"&gt;Video&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'../files/2025-04-24-orienting-yourself-in-the-visual-flow.mp4'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
      &lt;span class="n"&gt;html_attributes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"loop=True autoplay=True  controls=True"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[1]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;&lt;video autoplay="True" controls="True" loop="True" src="https://laurentperrinet.github.io/sciblog/files/2025-04-24-orienting-yourself-in-the-visual-flow.mp4"&gt;
      Your browser does not support the &lt;code&gt;video&lt;/code&gt; element.
    &lt;/video&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;And try out how to move that FoE with respect to the center of gaze:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Video&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'../files/2025-04-24-orienting-yourself-in-the-visual-flow-perturb.mp4'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
      &lt;span class="n"&gt;html_attributes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"loop=True autoplay=True  controls=True"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt output_prompt"&gt;Out[2]:&lt;/div&gt;
&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;&lt;video autoplay="True" controls="True" loop="True" src="https://laurentperrinet.github.io/sciblog/files/2025-04-24-orienting-yourself-in-the-visual-flow-perturb.mp4"&gt;
      Your browser does not support the &lt;code&gt;video&lt;/code&gt; element.
    &lt;/video&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In [3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="input_area"&gt;
&lt;div class="highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;pip&lt;/span&gt; install --upgrade pip
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="k"&gt;pip&lt;/span&gt;  install torch==2.7.0
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;
&lt;div class="output_area"&gt;
&lt;div class="prompt"&gt;&lt;/div&gt;
&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;Requirement already satisfied: pip in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (25.0.1)
Collecting pip
  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-25.2-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 25.0.1
    Uninstalling pip-25.0.1:
      Successfully uninstalled pip-25.0.1
Successfully installed pip-25.2
Note: you may need to restart the kernel to use updated packages.
Collecting torch==2.7.0
  Using cached torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)
Requirement already satisfied: filelock in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (3.18.0)
Requirement already satisfied: typing-extensions&amp;gt;=4.10.0 in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (4.13.0)
Requirement already satisfied: setuptools in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (75.6.0)
Collecting sympy&amp;gt;=1.13.3 (from torch==2.7.0)
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: networkx in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (3.4.2)
Requirement already satisfied: jinja2 in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (3.1.4)
Requirement already satisfied: fsspec in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from torch==2.7.0) (2025.3.0)
Requirement already satisfied: mpmath&amp;lt;1.4,&amp;gt;=1.1.0 in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from sympy&amp;gt;=1.13.3-&amp;gt;torch==2.7.0) (1.3.0)
Requirement already satisfied: MarkupSafe&amp;gt;=2.0 in /Users/laurentperrinet/sdrive_cnrs/blog/laurentperrinet.github.io_sciblog/.venv/lib/python3.13/site-packages (from jinja2-&amp;gt;torch==2.7.0) (3.0.2)
Using cached torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Installing collected packages: sympy, torch
  Attempting uninstall: sympy
    Found existing installation: sympy 1.13.1
    Uninstalling sympy-1.13.1:
      Successfully uninstalled sympy-1.13.1━━━━━ &lt;span class="ansi-green-fg"&gt;0/2&lt;/span&gt; [sympy]
  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━ &lt;span class="ansi-green-fg"&gt;0/2&lt;/span&gt; [sympy]
    Found existing installation: torch 2.6.0 &lt;span class="ansi-green-fg"&gt;0/2&lt;/span&gt; [sympy]
    Uninstalling torch-2.6.0:[0m&lt;span class="ansi-black-intense-fg"&gt;╺&lt;/span&gt;&lt;span class="ansi-black-intense-fg"&gt;━━━━━━━━━━━━━━━━━━━&lt;/span&gt; &lt;span class="ansi-green-fg"&gt;1/2&lt;/span&gt; [torch]
      Successfully uninstalled torch-2.6.0&lt;span class="ansi-black-intense-fg"&gt;━━━━━━━━━━━━━━━━━━━&lt;/span&gt; &lt;span class="ansi-green-fg"&gt;1/2&lt;/span&gt; [torch]
   &lt;span class="ansi-black-intense-fg"&gt;━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━&lt;/span&gt; &lt;span class="ansi-green-fg"&gt;2/2&lt;/span&gt; [torch]32m1/2 [torch]
&lt;span class="ansi-red-fg"&gt;ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.&lt;/span&gt;&lt;span class="ansi-red-fg"&gt;
&lt;/span&gt;Successfully installed sympy-1.14.0 torch-2.7.0
Note: you may need to restart the kernel to use updated packages.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2025-04-24-orienting-yourself-in-the-visual-flow.html"&gt;Read more…&lt;/a&gt; (18 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>behavior</category><category>blog</category><category>motion</category><category>motionclouds</category><category>psychophysics</category><category>python</category><category>retina</category><category>saccades</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2025-04-24-orienting-yourself-in-the-visual-flow.html</guid><pubDate>Thu, 24 Apr 2025 04:15:52 GMT</pubDate></item><item><title>Using Subsets in PyTorch</title><link>https://laurentperrinet.github.io/sciblog/posts/2025-03-26-using-subsets-in-pytorch.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Summary : This notebook demonstrates how to create and use subsets of datasets in PyTorch. Subsets are particularly useful for working with specific portions of your data (e.g., validation sets or smaller batches for experimentation). We will use the MNIST dataset as an example.&lt;/p&gt;
&lt;p&gt;Using subsets in PyTorch allows you to work with specific portions of your dataset efficiently. This is particularly useful for tasks like validation, testing, or when working with large datasets where loading the entire dataset into memory is impractical.&lt;/p&gt;
&lt;p&gt;Key Points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficiency: Using Subset avoids creating copies of your data, which saves memory and processing time.&lt;/li&gt;
&lt;li&gt;Flexibility: You can create multiple subsets from the same dataset for different tasks (e.g., validation, testing).&lt;/li&gt;
&lt;li&gt;Integration: Subsets work seamlessly with PyTorch’s DataLoader, making it easy to integrate into training loops.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By leveraging subsets in PyTorch, you can efficiently manage and experiment with portions of your data without compromising on performance or memory usage.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2025-03-26-using-subsets-in-pytorch.html"&gt;Read more…&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>machine-learning</category><category>python</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2025-03-26-using-subsets-in-pytorch.html</guid><pubDate>Wed, 26 Mar 2025 10:48:27 GMT</pubDate></item><item><title>La vibration des apparences</title><link>https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;À l’occasion de la Biennale d’Aix-en-Provence, dans le cadre de CHRONIQUES – Biennale des Imaginaires Numériques, l’association Arts Vivants présente au musée Granet, du 8 novembre 2024 au 19 janvier 2025, une exposition consacrée à l’artiste contemporain Étienne Rey et intitulée &lt;a href="https://laurentperrinet.github.io/post/2024-11-07_vibration-apparences/"&gt;La vibration des apparences&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Je décris ici le code utilisé pour générer une des oeuvres et qui a donné lieu à l'affiche.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html"&gt;Read more…&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>grand-public</category><category>math</category><category>nature</category><category>numpy</category><category>open-science</category><category>outreach</category><category>python</category><category>trames</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html</guid><pubDate>Sat, 18 Jan 2025 09:00:53 GMT</pubDate></item><item><title>Understanding Image Normalization in CNNs</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Architectural innovations in deep learning occur at a breakneck pace, yet fragments of legacy code often persist, carrying assumptions and practices whose necessity remains unquestioned. Practitioners frequently accept these inherited elements as optimal by default, rarely stopping to reevaluate their continued relevance or efficiency.&lt;/p&gt;
&lt;p&gt;Input normalization for convolutional neural networks, particularly in image processing, is a prime example of these unscrutinized practices. This is especially evident in the widespread use of pre-trained models like VGG or ResNet, where specific normalization values have become standard across the community. In particular, the standard values used for ImageNet training are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean: &lt;code&gt;[0.485, 0.456, 0.406]&lt;/code&gt; (for R,G,B channels respectively)&lt;/li&gt;
&lt;li&gt;Std: &lt;code&gt;[0.229, 0.224, 0.225]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is the origin of these values? Are they really important?&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.14602370"&gt;&lt;img alt="DOI" src="https://zenodo.org/badge/DOI/10.5281/zenodo.14602370.svg"&gt;&lt;/a&gt;
&lt;a href="https://github.com/laurentperrinet/2024-12-09-normalizing-images-in-convolutional-neural-networks"&gt;&lt;img alt="GitHub" src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html"&gt;Read more…&lt;/a&gt; (36 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>learning</category><category>machine-learning</category><category>numpy</category><category>open-science</category><category>python</category><category>pytorch</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html</guid><pubDate>Mon, 09 Dec 2024 15:00:53 GMT</pubDate></item><item><title>De qui parle-t-on quand on parle d'IA ?</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-11-26_De-qui-parle-t-on-quand-on-parle-d-IA.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;p&gt;L'intelligence artificielle (IA) fait les gros titres des media depuis qu'elle a envahi notre quotidien. Si elle nous aide à rédiger un document ou à choisir la prochaine musique que nous allons entendre, elle peut aussi contribuer à la création de nouveaux médicaments. Plus généralement, elle est à l'origine d'une révolution dans divers domaines tels que les transports, l'industrie, le commerce et les services. Elle est si omniprésente qu'elle tient aujourd'hui une place à part, à tel point qu'on a pris l'habitude de la nommer par ce nom propre : « l'IA ».&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2024-11-26_De-qui-parle-t-on-quand-on-parle-d-IA.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>blog</category><category>grand-public</category><category>neuroai</category><category>outreach</category><category>sciblog</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-11-26_De-qui-parle-t-on-quand-on-parle-d-IA.html</guid><pubDate>Tue, 26 Nov 2024 13:40:39 GMT</pubDate></item><item><title>Using singularity on slurm</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I will disp)lay my configuration that I use to run &lt;a href="https://sylabs.io/singularity/"&gt;Singularity&lt;/a&gt; on a &lt;a href="https://slurm.schedmd.com/"&gt;SLURM&lt;/a&gt; cluster. &lt;/p&gt;
&lt;p&gt;My use case is mostly using a recent version of pyTorch on GPUs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>open-science</category><category>shell</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html</guid><pubDate>Fri, 12 Jan 2024 13:40:39 GMT</pubDate></item><item><title>A textured Ouchi Illusion</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The &lt;a href="https://mathworld.wolfram.com/OuchiIllusion.html"&gt;Ouchi illusion&lt;/a&gt; is a powerful demonstration that static images may produce an illusory movement. One striking aspect is that it makes you feel quite dizzy from trying to compensate for this illusory movement.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ouchi.jpg" src="https://mathworld.wolfram.com/images/gifs/ouchi.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The illlusion is is generated by your own eye movements and is a consequence of the &lt;a href="https://en.wikipedia.org/wiki/Aperture_problem"&gt;aperture problem&lt;/a&gt;, which is a fundamental problem in vision science. The aperture problem is the fact that the visual system can only integrate information along the direction of motion, and not perpendicular to it. This is because the visual system is made of a set of filters that are oriented in different directions, and the integration is done by summing the responses of these filters. The aperture problem is a problem because it means that the visual system cannot recover the direction of motion of a contour from the responses of these filters.&lt;/p&gt;
&lt;p&gt;Here, we explore variations of this illusion which xwould use textures instead of regular angles using the &lt;a href="https://github.com/NeuralEnsemble/MotionClouds"&gt;MotionClouds&lt;/a&gt; library. The idea is to use the same texture in the two parts of the image (center vs surround), but to rotate by 90° the texture in the center:&lt;/p&gt;
&lt;p&gt;&lt;img alt="my sweet ouchi" src="https://laurentperrinet.github.io/sciblog/files/2023-11-29-ouchi-illusion.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimizing the parameters of the texture would help tell us what matters to generate that illusion...&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>motionclouds</category><category>orientation</category><category>python</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html</guid><pubDate>Wed, 29 Nov 2023 10:54:45 GMT</pubDate></item><item><title>Modelling wind ripples</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Here, I try to simulate the patterns obtained on a sandy terrain. This is what is observed when the wind blows on the dry surface of a land with sand or also on the surface of the sea in the presence of currents.&lt;/p&gt;
&lt;p&gt;As described in &lt;a href="https://en.wikipedia.org/wiki/Dune"&gt;https://en.wikipedia.org/wiki/Dune&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When a sandy seabed is subject to wave action and the wave orbital motion is strong enough to move sand grains, ripples often appear. The ripples induced by wave action are called “wave ripples”; their characteristics being different from those of the ripples generated by steady flows. The most striking difference between wave ripple fields and current ripple fields is the regularity of the former. Indeed, regular long-crested wave ripple fields are often observed on tidal beaches from which the sea has withdrawn at low water (see figure 1).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A nice example is shown in &lt;a href="http://www.coastalwiki.org/wiki/Wave_ripple_formation"&gt;http://www.coastalwiki.org/wiki/Wave_ripple_formation&lt;/a&gt; showing&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ripples observed at Sea Rim State Park, along the coast of east Texas close to the border with Louisiana (courtesy by Zoltan Sylvester).
&lt;img alt="" src="http://www.coastalwiki.org/w/images/6/65/WaveRippleFormationFig0.jpg"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://www.coastalwiki.org/wiki/Wave_ripple_formation"&gt;Wave ripple formation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An interesting aspect of that patterns is that they may occur at different scales, like taht example on the surface of the Mars planet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Overview of large sand wave field and high-resolution difference map of two surveys approximately 21 hours apart illustrating both large-scale and small-scale sand wave migration and orientation. Migration is from right to left.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="https://www.researchgate.net/profile/Daniel-Hanes/publication/252161559/figure/fig1/AS:669991225552924@1536749764962/Overview-of-large-sand-wave-field-and-high-resolution-difference-map-of-two-surveys.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>nature</category><category>numpy</category><category>open-science</category><category>python</category><category>trames</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html</guid><pubDate>Sun, 16 Apr 2023 17:30:21 GMT</pubDate></item><item><title>Implementing a retinotopic transform using `grid_sample` from pyTorch</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Implementing-a-retinotopic-transform-using-grid_sample-from-pyTorch"&gt;Implementing a retinotopic transform using &lt;code&gt;grid_sample&lt;/code&gt; from &lt;code&gt;pyTorch&lt;/code&gt;&lt;a class="anchor-link" href="https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html#Implementing-a-retinotopic-transform-using-grid_sample-from-pyTorch"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html"&gt;grid_sample&lt;/a&gt; transform is a powerful function which allows to transform any input image into a new topology. It is notably used in &lt;a href="https://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Networks&lt;/a&gt; for instance to learn CNN to be invariant to affine transforms. We used it recently in a publication &lt;a href="https://laurentperrinet.github.io/publication/dabane-22/"&gt;What You See Is What You Transform: Foveated Spatial Transformers as a Bio-Inspired Attention Mechanism&lt;/a&gt; by Ghassan Dabane &lt;em&gt;et al&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The use of &lt;code&gt;grid_sample&lt;/code&gt; can b etedious and here, we show how to use it to create a log-polar transform of the image and create the following figure:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Retinotopy" src="https://laurentperrinet.github.io/sciblog/files/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A picture (extract from the painting &lt;a href="https://w.wiki/AckL"&gt;"The Ambassadors" by Hans Holbein the Younger&lt;/a&gt; can be represented on a regular grid represented by vertical (red) and horizontal (blue) lines. Retinotopy transforms this grid, and in particular the area representing the fovea (shaded gray) is over-represented. Applied to the original image of the portrait, the image is strongly distorted and represents more finally the parts under the axis of sight (here the mouth).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>machine-learning</category><category>open-science</category><category>python</category><category>pytorch</category><category>retina</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html</guid><pubDate>Thu, 02 Feb 2023 07:59:14 GMT</pubDate></item></channel></rss>