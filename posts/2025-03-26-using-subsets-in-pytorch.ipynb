{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary : This notebook demonstrates how to create and use subsets of datasets in PyTorch. Subsets are particularly useful for working with specific portions of your data (e.g., validation sets or smaller batches for experimentation). We will use the MNIST dataset as an example. \n",
    "\n",
    "Using subsets in PyTorch allows you to work with specific portions of your dataset efficiently. This is particularly useful for tasks like validation, testing, or when working with large datasets where loading the entire dataset into memory is impractical. \n",
    "\n",
    "\n",
    "Key Points: \n",
    "\n",
    " * Efficiency: Using Subset avoids creating copies of your data, which saves memory and processing time.\n",
    " * Flexibility: You can create multiple subsets from the same dataset for different tasks (e.g., validation, testing).\n",
    " * Integration: Subsets work seamlessly with PyTorch’s DataLoader, making it easy to integrate into training loops.\n",
    "     \n",
    "\n",
    "By leveraging subsets in PyTorch, you can efficiently manage and experiment with portions of your data without compromising on performance or memory usage. \n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set random seed for reproducibility (good practice)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load the Full Dataset\n",
    "\n",
    "We start by loading the full MNIST dataset. The `MNIST` class from `torchvision.datasets` handles downloading and preprocessing the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 60000 samples\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset parameters\n",
    "root = '/tmp/data'  # Root directory where data will be stored\n",
    "train = True      # Use training data (set to False for test data)\n",
    "transform = transforms.ToTensor()  # Convert images to tensors\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = datasets.MNIST(root=root, train=train, transform=transform, download=True)\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(f\"Full dataset size: {len(full_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Create a Subset of the Dataset\n",
    "\n",
    "Use the `Subset` class to create smaller portions of your dataset. Here, we create a subset containing the first 100 samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 100 samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Define the indices for the subset (first 100 samples)\n",
    "subset_indices = list(range(100))  # You can modify this range as needed\n",
    "\n",
    "# Create the subset dataset\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "# Print information about the subset\n",
    "print(f\"Subset size: {len(subset_dataset)} samples\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Load the Subset Using DataLoader\n",
    "\n",
    "Load the subset into batches using a `DataLoader`. This is useful for training loops or data processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset DataLoader: 4 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for loading the data\n",
    "batch_size = 32\n",
    "shuffle = True  # Shuffle the data in each epoch\n",
    "\n",
    "# Create the DataLoader for the subset\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Print information about the DataLoader\n",
    "print(f\"Subset DataLoader: {len(subset_loader)} batches per epoch\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Iterate Over the Subset\n",
    "\n",
    "Use a simple loop to iterate over the subset and retrieve batches of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Batch 0: Loss: 2.2630\n",
      "\n",
      "Epoch 2\n",
      "Batch 0: Loss: 2.2093\n",
      "\n",
      "Epoch 3\n",
      "Batch 0: Loss: 2.2010\n",
      "\n",
      "Epoch 4\n",
      "Batch 0: Loss: 2.0449\n",
      "\n",
      "Epoch 5\n",
      "Batch 0: Loss: 2.0448\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a model (example: simple linear classifier)\n",
    "input_size = 28 * 28  # MNIST images are 28x28\n",
    "output_size = 10      # There are 10 classes in MNIST\n",
    "\n",
    "model = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (example: one epoch)\n",
    "for epoch in range(5):\n",
    "    print(f'\\nEpoch {epoch + 1}')\n",
    "    for batch_idx, (data, labels) in enumerate(subset_loader):\n",
    "        # Flatten the images\n",
    "        data = data.view(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}: Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\nTraining complete!\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Optional - Create Multiple Subsets for Different Purposes\n",
    "\n",
    "You can create multiple subsets for different purposes (e.g., training, validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation subset size: 100 samples\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a validation subset\n",
    "val_indices = list(range(100, 200))  # Next 100 samples as validation set\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "print(f\"Validation subset size: {len(val_dataset)} samples\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Conclusion\n",
    "\n",
    "You’ve now seen how to create and use subsets of a PyTorch dataset. Subsets are super handy for:  \n",
    "- Playing around with smaller chunks of data.  \n",
    "- Setting up validation sets or custom splits.  \n",
    "- Keeping things light on memory when dealing with big datasets.  \n",
    "\n",
    "## An alternative: `random_split`\n",
    "\n",
    "For cross-validation, one may also simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = .8\n",
    "len_dataset = len(full_dataset)\n",
    "len_train = int(rho*len_dataset)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(range(len_dataset),\n",
    "                                                           [len_train, len_dataset-len_train], \n",
    "                                                           generator=torch.Generator().manual_seed(42),\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More things to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=torch.Generator().manual_seed(42)\n",
    "sampler = torch.utils.data.RandomSampler(full_dataset, replacement=True, num_samples=10000, generator=generator)\n",
    "train_loader = torch.utils.data.DataLoader(full_dataset, batch_size=32, \n",
    "                                           sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some book keeping for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.2\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "torch      : 2.6.0\n",
      "torchvision: 0.21.0\n",
      "\n",
      "Compiler    : Clang 16.0.0 (clang-1600.0.26.6)\n",
      "OS          : Darwin\n",
      "Release     : 24.3.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 10\n",
      "Architecture: 64bit\n",
      "\n",
      "Hostname: obiwan.local\n",
      "\n",
      "Git hash: 83fd93189577c8e6ac256a705efe435477eebe18\n",
      "\n",
      "Git repo: https://github.com/laurentperrinet/sciblog\n",
      "\n",
      "Git branch: master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -i -h -m -v -p torch,torchvision  -r -g -b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
