<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scientific logbook</title><link>https://laurentperrinet.github.io/sciblog/</link><description>This is my scientific logbook. Experiments happen here in an apparently random order, most of the time with more questions than answers... Do not hesitate to comment!</description><atom:link href="https://laurentperrinet.github.io/sciblog/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:laurent.perrinet@univ-amu.fr"&gt;Laurent Perrinet&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/2.5/ar/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="http://i.creativecommons.org/l/by-nc-sa/2.5/ar/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Mon, 10 Mar 2025 07:49:25 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>La vibration des apparences</title><link>https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;À l’occasion de la Biennale d’Aix-en-Provence, dans le cadre de CHRONIQUES – Biennale des Imaginaires Numériques, l’association Arts Vivants présente au musée Granet, du 8 novembre 2024 au 19 janvier 2025, une exposition consacrée à l’artiste contemporain Étienne Rey et intitulée &lt;a href="https://laurentperrinet.github.io/post/2024-11-07_vibration-apparences/"&gt;La vibration des apparences&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Je décris ici le code utilisé pour générer une des oeuvres et qui a donné lieu à l'affiche.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html"&gt;Read more…&lt;/a&gt; (32 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>grand-public</category><category>math</category><category>nature</category><category>numpy</category><category>open-science</category><category>outreach</category><category>python</category><category>trames</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2025-01-18_la-vibration-des-apparences.html</guid><pubDate>Sat, 18 Jan 2025 09:00:53 GMT</pubDate></item><item><title>Understanding Image Normalization in CNNs</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Architectural innovations in deep learning occur at a breakneck pace, yet fragments of legacy code often persist, carrying assumptions and practices whose necessity remains unquestioned. Practitioners frequently accept these inherited elements as optimal by default, rarely stopping to reevaluate their continued relevance or efficiency.&lt;/p&gt;
&lt;p&gt;Input normalization for convolutional neural networks, particularly in image processing, is a prime example of these unscrutinized practices. This is especially evident in the widespread use of pre-trained models like VGG or ResNet, where specific normalization values have become standard across the community. In particular, the standard values used for ImageNet training are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean: &lt;code&gt;[0.485, 0.456, 0.406]&lt;/code&gt; (for R,G,B channels respectively)&lt;/li&gt;
&lt;li&gt;Std: &lt;code&gt;[0.229, 0.224, 0.225]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is the origin of these values? Are they really important?&lt;/p&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.5281/zenodo.14602370"&gt;&lt;img alt="DOI" src="https://zenodo.org/badge/DOI/10.5281/zenodo.14602370.svg"&gt;&lt;/a&gt;
&lt;a href="https://github.com/laurentperrinet/2024-12-09-normalizing-images-in-convolutional-neural-networks"&gt;&lt;img alt="GitHub" src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html"&gt;Read more…&lt;/a&gt; (36 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>learning</category><category>machine-learning</category><category>numpy</category><category>open-science</category><category>python</category><category>pytorch</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-12-09-normalizing-images-in-convolutional-neural-networks.html</guid><pubDate>Mon, 09 Dec 2024 15:00:53 GMT</pubDate></item><item><title>De qui parle-t-on quand on parle d'IA ?</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-11-26_De-qui-parle-t-on-quand-on-parle-d-IA.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;h2&gt;De qui parle-t-on quand on parle d'IA ?&lt;/h2&gt;
&lt;p&gt;L'intelligence artificielle (IA) fait les gros titres des media depuis qu'elle a envahi notre quotidien. Si elle nous aide à rédiger un document ou à choisir la prochaine musique que nous allons entendre, elle peut aussi contribuer à la création de nouveaux médicaments. Plus généralement, elle est à l'origine d'une révolution dans divers domaines tels que les transports, l'industrie, le commerce et les services. Elle est si omniprésente qu'elle tient aujourd'hui une place à part, à tel point qu'on a pris l'habitude de la nommer par ce nom propre : « l'IA ».&lt;/p&gt;
&lt;h3&gt;« Who am AI ? »&lt;/h3&gt;
&lt;p&gt;De façon souvent amusée, on la représente souvent sur la couverture des journaux ou livres spécialisés sous les traits d'un robot à l'apparence humaine, lui octroyant tantôt une personnalité positive, tantôt une personnalité effrayante. C'est oublier que depuis déjà quelques décennies, des hordes de robots-outils équipés d'intelligence artificielle occupent nos usines et automatisent la fabrication d'objets complexes, des vêtements aux voitures, en passant par toute l’électronique moderne, comme l'appareil sur lequel vous lisez sans doute ces mots. C'est avec l'émergence des agents conversationnels, popularisés par ChatGPT, qu'une nouvelle étape a été franchie. Cette IA ne contrôle plus des objets mécaniques, mais se rapproche distinctement des capacités humaines en matière de création de contenu. &lt;/p&gt;
&lt;p&gt;L'une des conséquences de ce changement de paradigme est la personnification de cette technologie, de sorte qu'il n'est pas rare d'entendre dire : « Donnons ce problème à résoudre à l'IA » ou encore « C'est la faute de l'IA ! ». Prenons alors du recul pour découvrir ce qui se cache derrière ce terme. Par définition, l'IA est un domaine de la recherche scientifique qui vise à créer des machines (robotiques ou logicielles) capables d'accomplir des tâches généralement considérées comme relevant de l'intelligence humaine. Dans ce but, elle regroupe des disciplines telles que la robotique, l'informatique, les mathématiques et les neurosciences. Ces systèmes s'appuient sur des techniques d'apprentissage utilisant des bases de données massives avec pour but de simuler la réalisation de tâches données. L'IA ne se limite donc pas à une seule forme, mais se décline en d'innombrables applications.&lt;/p&gt;
&lt;h3&gt;L’intelligence artificielle reste avant tout une machine.&lt;/h3&gt;
&lt;p&gt;En plongeant dans les rouages de ces systèmes complexes, souvent paramétrés par des milliards de réglages minutieux, on se rend compte que si ces technologies sont simplement des machines, elles sont redoutablement efficaces. En microbiologie, le système AlphaFold révolutionne le domaine en permettant de prévoir la configuration spatiale des protéines en quelques minutes seulement, alors que cette tâche prenait plusieurs jours il y a quelques années. En neurosciences, les progrès récents de l'apprentissage automatique permettent d'interagir avec les signaux d'activité neurale de telle sorte que, dans les années à venir, des personnes puissent communiquer par la pensée. On peut aussi se prendre à rêver qu'un jour, il deviendra possible d'étendre cette IA à d'autres espèces vivantes, comme les chats ou les oiseaux, ouvrant ainsi la voie à une communication inter-espèce qui bouleverserait notre &lt;a href="https://trustmyscience.com/intelligence-artificielle-parler-aux-animaux/"&gt;rapport&lt;/a&gt; à l'&lt;a href="https://www.scientificamerican.com/article/how-scientists-are-using-ai-to-talk-to-animals/"&gt;environnement&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Les domaines d'application sont infinis ! On comprend alors mieux qu'à l'aune de ces découvertes, ces technologies impliquent des enjeux majeurs pour nos sociétés. Ce n'est pas sans conséquences. D'ores et déjà, les visages dans les publicités sont majoritairement générés de façon totalement artificielle et &lt;a href="https://blog.hubspot.fr/marketing/ia-dans-campagnes-marketing-pub"&gt;ne correspondent plus à des personnes réelles&lt;/a&gt; et, d'après certaines estimations, l'IA générative créera en 2026 &lt;a href="https://www.e-marketing.fr/Thematique/influences-1293/reseaux-sociaux-2216/Breves/Pres-de-50-du-contenu-sur-les-reseaux-sociaux-devraient-461932.htm"&gt;plus de la moitié du contenu sur les réseaux sociaux&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Une société transformée par l’intelligence artificielle.&lt;/h3&gt;
&lt;p&gt;L'IA est un outil transformateur, mais son usage généralisé représente un défi et soulève de nombreuses questions éthiques. En premier lieu, comme l’IA est majoritairement le résultat d’un apprentissage à partir de bases de données de productions humaines (photos, sons, textes, etc.), elle a tendance à reproduire les biais humains, tels que ceux liés au genre, à l'origine sociale ou à la race. Sans corriger ces biais, son utilisation indiscriminée peut contribuer à créer un cercle vicieux qui amplifie les préjugés existants, jusqu’à créer des « chambres d’écho ». Ce système s'auto-amplifiant, il conduit potentiellement à la création de réalités alternatives, comme les fameuses « fake news ». &lt;/p&gt;
&lt;p&gt;C'est d'autant plus dangereux que son utilisation donne l'illusion d'une discussion avec &lt;a href="https://theconversation.com/chatgpt-ma-dit-que-lillusion-de-la-discussion-avec-lia-nous-mene-a-lerreur-238443"&gt;un autre être qui penserait comme nous&lt;/a&gt;. Ainsi, il a été démontré que des utilisateurs de ChatGPT ont davantage confiance en cette IA qu'en des êtres humains, &lt;a href="https://www.futura-sciences.com/tech/actualites/technologie-utilisateurs-ont-plus-plus-echanges-intimes-chatgpt-cela-inquiete-maison-mere-115214/"&gt;ce qui peut les amener à changer d'avis, potentiellement sur des questions politiques&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;Quel contrôle sur l'IA ?&lt;/h3&gt;
&lt;p&gt;Allons encore plus loin : une majorité de ces technologies sont développées par des sociétés privées qui gardent leurs IA secrètes. OpenAI, le développeur de ChatGPT, a utilisé une part non négligeable du contenu total d'Internet, en particulier des sources libres comme Wikipédia. Cependant, cette société commerciale n'a pas divulgué les détails précis de ses processus d'entraînement et de modération. Les objectifs de cette intelligence artificielle ne sont donc pas nécessairement alignés sur ceux de l'entreprise en termes d'éthique, et le secret qui entoure son développement pose un défi quant à son évaluation et à son contrôle pour éviter des dérives potentielles. &lt;/p&gt;
&lt;p&gt;Ce constat est d'autant plus inquiétant que la majorité des technologies d'IA se trouvent actuellement entre les mains de grandes entreprises telles que Google, Amazon, Facebook, Apple et Microsoft (les célèbres GAFAM), alors que &lt;a href="https://theconversation.com/lechiquier-mondial-de-lia-entre-regulations-et-soft-power-233387"&gt;des systèmes de régulation peinent à se mettre en place à l'échelle mondiale&lt;/a&gt;. Le contexte de &lt;a href="https://legrandcontinent.eu/fr/2024/11/21/le-retour-de-trump-menace-les-efforts-visant-a-rendre-lia-plus-sure/"&gt;la dernière élection états-unienne n'est pas pour autant encourageant&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Le respect de la vie privée et la surveillance de masse rendue possible par l'intelligence artificielle constituent alors un enjeu vital. Il est essentiel de veiller à ce que ces IA ne soient pas utilisées à des fins de surveillance, comme &lt;a href="https://gizmodo.com/mozilla-new-cars-data-privacy-report-1850805416"&gt;cela se généralise dans l'espace public ou dans les voitures récentes&lt;/a&gt;. Ceci requiert une vigilance constante pour obtenir de meilleures garanties.&lt;/p&gt;
&lt;h2&gt;Éviter le sentiment d'abandon des utilisateurs d'IA&lt;/h2&gt;
&lt;p&gt;Cependant, avant d'y parvenir, il reste encore un long chemin à parcourir. Il est alors intéressant de comprendre pourquoi cette technologie nous semble souvent si étrange et inquiétante. Certes, l'IA est une technologie complexe et en constante innovation qui parait impossible à appréhender. Face à cette complexité d'apparence insurmontable, nombre d’utilisateurs se sentent dépassés et choisissent alors de l'utiliser aveuglément, espérant simplement en tirer avantage et abandonnant l'idée d'en considérer les risques potentiels.&lt;/p&gt;
&lt;p&gt;En utilisant l'IA de cette manière, nous livrons à notre insu et sans contrepartie une énorme richesse &lt;a href="https://medium.com/illumination/ms-word-is-using-you-to-train-ai-86d6a4d87021?sk=b9193bd978b48741d4778ad003cce716"&gt;en livrant des documents à caractère personnels comme nos documents de travail ou l'ensemble de nos interactions avec ces outils&lt;/a&gt;. Ces données sont ensuite valorisées pour améliorer les modèles d'IA et promouvoir de nouveaux produits, mais sans forcément en connaître la finalité. Qui sait que &lt;a href="https://www.reuters.com/technology/tesla-workers-shared-sensitive-images-recorded-by-customer-cars-2023-04-06/"&gt;les employés de Tesla se partageaient les images croustillantes qu'ils pouvaient obtenir dans les voitures de la marque&lt;/a&gt; ?&lt;/p&gt;
&lt;h3&gt;La nécessité de former l'IA à notre image&lt;/h3&gt;
&lt;p&gt;Dans un objectif de souveraineté et de confidentialité de nos propres données, il est essentiel de passer à un nouveau modèle de fonctionnement pour garantir qu'elles ne soient pas exploitées à des fins d'espionnage industriel ou de ciblage publicitaire. Heureusement, les solutions existent. Il est concrètement possible de promouvoir des ressources libres de droit, comme les agents conversationnels développés par Mistral AI qui sont fournis avec tous leurs paramètres, et de favoriser une utilisation locale, en dehors d'un cloud géré par un GAFAM, ce qui est possible sur un ordinateur personnel grâce à GPT4all (https://www.nomic.ai/gpt4all).&lt;/p&gt;
&lt;p&gt;Définir des objectifs pour l'IA implique également de veiller à ce que les systèmes soient conçus dans une perspective de diversité culturelle et sociale. On peut par exemple tenir compte des expériences et des besoins des personnes présentant des divergences neurologiques, comme celles dans le spectre autistique. Le véritable défi réside alors dans la nécessité de veiller à ce que toute intelligence artificielle soit développée, utilisée et réglementée de manière responsable, afin de garantir une utilisation équitable et respectueuse des droits humains, tout en évitant de la brider excessivement. &lt;/p&gt;
&lt;h2&gt;l'IA... ça s'apprend !&lt;/h2&gt;
&lt;p&gt;Pour atteindre cet objectif, il convient de promouvoir la diffusion large et accessible de l'IA dès l'école, ainsi que de sensibiliser le grand public au fonctionnement de cette technologie. À ce titre, pourquoi ne pas créer pour les enfants une nouvelle discipline à l'école, au même titre que les mathématiques, l'histoire ou le français, et qui soit entièrement dévolue à l’IA ? On peut aussi s'inspirer de Singapour où &lt;a href="https://www.channelnewsasia.com/singapore/ai-talent-15000-jobs-training-education-national-strategy-3974591"&gt;une formation est proposée à tous les salariés en activité ou non pour se former à l'IA et à ses métiers&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Enseigner l'IA permettra aux utilisateurs de ne pas considérer la machine avec laquelle ils interagissent comme une entité propre et hors de portée, mais bien comme une création humaine, avec toutes ses diversités. Et l'IA, malgré ses progrès impressionnants, ne peut jamais remplacer notre responsabilité, car elle reste une création humaine, conçue par des individus et des groupes qui en déterminent les choix et les objectifs. &lt;/p&gt;
&lt;p&gt;En quelque sorte, si l'on représente souvent l'IA par un humanoïde, c'est le révélateur d'un vrai désir de la rendre plus proche de nos propres aspirations. Si ce n'est sûrement pas l'objectif de certaines sociétés privées, il existe de nombreuses solutions pour réaliser cette transformation positive de la société. En ce sens, en prenant conscience de notre tendance trompeuse à l'anthropomorphisation, l'IA perd de son aura d'humanoïde inquiétant et se révèle simplement comme un outil dont l'utilisation face aux enjeux sociétaux repose entre nos mains.&lt;/p&gt;</description><category>blog</category><category>grand-public</category><category>neuroai</category><category>outreach</category><category>sciblog</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-11-26_De-qui-parle-t-on-quand-on-parle-d-IA.html</guid><pubDate>Tue, 26 Nov 2024 13:40:39 GMT</pubDate></item><item><title>Using singularity on slurm</title><link>https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div&gt;&lt;p&gt;In this post, I will disp)lay my configuration that I use to run &lt;a href="https://sylabs.io/singularity/"&gt;Singularity&lt;/a&gt; on a &lt;a href="https://slurm.schedmd.com/"&gt;SLURM&lt;/a&gt; cluster. &lt;/p&gt;
&lt;p&gt;My use case is mostly using a recent version of pyTorch on GPUs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>open-science</category><category>shell</category><guid>https://laurentperrinet.github.io/sciblog/posts/2024-01-12-singularity-on-slurm.html</guid><pubDate>Fri, 12 Jan 2024 13:40:39 GMT</pubDate></item><item><title>A textured Ouchi Illusion</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;The &lt;a href="https://mathworld.wolfram.com/OuchiIllusion.html"&gt;Ouchi illusion&lt;/a&gt; is a powerful demonstration that static images may produce an illusory movement. One striking aspect is that it makes you feel quite dizzy from trying to compensate for this illusory movement.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ouchi.jpg" src="https://mathworld.wolfram.com/images/gifs/ouchi.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The illlusion is is generated by your own eye movements and is a consequence of the &lt;a href="https://en.wikipedia.org/wiki/Aperture_problem"&gt;aperture problem&lt;/a&gt;, which is a fundamental problem in vision science. The aperture problem is the fact that the visual system can only integrate information along the direction of motion, and not perpendicular to it. This is because the visual system is made of a set of filters that are oriented in different directions, and the integration is done by summing the responses of these filters. The aperture problem is a problem because it means that the visual system cannot recover the direction of motion of a contour from the responses of these filters.&lt;/p&gt;
&lt;p&gt;Here, we explore variations of this illusion which xwould use textures instead of regular angles using the &lt;a href="https://github.com/NeuralEnsemble/MotionClouds"&gt;MotionClouds&lt;/a&gt; library. The idea is to use the same texture in the two parts of the image (center vs surround), but to rotate by 90° the texture in the center:&lt;/p&gt;
&lt;p&gt;&lt;img alt="my sweet ouchi" src="https://laurentperrinet.github.io/sciblog/files/2023-11-29-ouchi-illusion.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimizing the parameters of the texture would help tell us what matters to generate that illusion...&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>motionclouds</category><category>orientation</category><category>python</category><category>v1</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-11-29-ouchi-illusion.html</guid><pubDate>Wed, 29 Nov 2023 10:54:45 GMT</pubDate></item><item><title>Modelling wind ripples</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Here, I try to simulate the patterns obtained on a sandy terrain. This is what is observed when the wind blows on the dry surface of a land with sand or also on the surface of the sea in the presence of currents.&lt;/p&gt;
&lt;p&gt;As described in &lt;a href="https://en.wikipedia.org/wiki/Dune"&gt;https://en.wikipedia.org/wiki/Dune&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When a sandy seabed is subject to wave action and the wave orbital motion is strong enough to move sand grains, ripples often appear. The ripples induced by wave action are called “wave ripples”; their characteristics being different from those of the ripples generated by steady flows. The most striking difference between wave ripple fields and current ripple fields is the regularity of the former. Indeed, regular long-crested wave ripple fields are often observed on tidal beaches from which the sea has withdrawn at low water (see figure 1).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A nice example is shown in &lt;a href="http://www.coastalwiki.org/wiki/Wave_ripple_formation"&gt;http://www.coastalwiki.org/wiki/Wave_ripple_formation&lt;/a&gt; showing&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ripples observed at Sea Rim State Park, along the coast of east Texas close to the border with Louisiana (courtesy by Zoltan Sylvester).
&lt;img alt="" src="http://www.coastalwiki.org/w/images/6/65/WaveRippleFormationFig0.jpg"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://www.coastalwiki.org/wiki/Wave_ripple_formation"&gt;Wave ripple formation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An interesting aspect of that patterns is that they may occur at different scales, like taht example on the surface of the Mars planet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Overview of large sand wave field and high-resolution difference map of two surveys approximately 21 hours apart illustrating both large-scale and small-scale sand wave migration and orientation. Migration is from right to left.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="https://www.researchgate.net/profile/Daniel-Hanes/publication/252161559/figure/fig1/AS:669991225552924@1536749764962/Overview-of-large-sand-wave-field-and-high-resolution-difference-map-of-two-surveys.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>nature</category><category>numpy</category><category>open-science</category><category>python</category><category>trames</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-04-16-modelling-wind-ripples.html</guid><pubDate>Sun, 16 Apr 2023 17:30:21 GMT</pubDate></item><item><title>Implementing a retinotopic transform using `grid_sample` from pyTorch</title><link>https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Implementing-a-retinotopic-transform-using-grid_sample-from-pyTorch"&gt;Implementing a retinotopic transform using &lt;code&gt;grid_sample&lt;/code&gt; from &lt;code&gt;pyTorch&lt;/code&gt;&lt;a class="anchor-link" href="https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html#Implementing-a-retinotopic-transform-using-grid_sample-from-pyTorch"&gt;¶&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html"&gt;grid_sample&lt;/a&gt; transform is a powerful function which allows to transform any input image into a new topology. It is notably used in &lt;a href="https://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Networks&lt;/a&gt; for instance to learn CNN to be invariant to affine transforms. We used it recently in a publication &lt;a href="https://laurentperrinet.github.io/publication/dabane-22/"&gt;What You See Is What You Transform: Foveated Spatial Transformers as a Bio-Inspired Attention Mechanism&lt;/a&gt; by Ghassan Dabane &lt;em&gt;et al&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The use of &lt;code&gt;grid_sample&lt;/code&gt; can b etedious and here, we show how to use it to create a log-polar transform of the image and create the following figure:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Retinotopy" src="https://laurentperrinet.github.io/sciblog/files/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A picture (extract from the painting &lt;a href="https://w.wiki/AckL"&gt;"The Ambassadors" by Hans Holbein the Younger&lt;/a&gt; can be represented on a regular grid represented by vertical (red) and horizontal (blue) lines. Retinotopy transforms this grid, and in particular the area representing the fovea (shaded gray) is over-represented. Applied to the original image of the portrait, the image is strongly distorted and represents more finally the parts under the axis of sight (here the mouth).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>machine-learning</category><category>open-science</category><category>python</category><category>pytorch</category><category>retina</category><category>vision</category><guid>https://laurentperrinet.github.io/sciblog/posts/2023-02-02-implementing-a-retinotopic-transform-using-grid_sample-from-pytorch.html</guid><pubDate>Thu, 02 Feb 2023 07:59:14 GMT</pubDate></item><item><title>Elections présidentielles 2022: estimation du transfert de voix</title><link>https://laurentperrinet.github.io/sciblog/posts/2022-06-08-transfert-de-voix.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;tl;dr : &lt;em&gt;On essaie ici de deviner le transfert des voix entre les choix effectués entre deux scrutins d'un vote (ici les élections présidentielles 2022 en France) par une méthode d'&lt;a href="https://fr.wikipedia.org/wiki/Apprentissage_automatique"&gt;apprentissage automatique&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/488514016"&gt;&lt;img alt="DOI" src="https://zenodo.org/badge/488514016.svg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Afin d'analyser les résultats des élections, par exemple les dernières élections présidentielles de 2022 en France, et de mieux comprendre la dynamique des choix de vote entre les différents groupes de population, il peut être utile d'utiliser des outils d'&lt;a href="https://fr.wikipedia.org/wiki/Apprentissage_automatique"&gt;apprentissage automatique&lt;/a&gt; pour inférer des structures à première vue cachées dans la masse des données. En particulier, inspiré par cet &lt;a href="https://www.lemonde.fr/les-decodeurs/article/2022/05/04/election-presidentielle-2022-quels-reports-de-voix-entre-les-deux-tours_6124672_4355770.html"&gt;article du Monde&lt;/a&gt;, on peut se poser la question de savoir &lt;em&gt;si on peut extraire depuis les données brutes des élections une estimation du report de voix entre les choix de vote au premier tour et ceux qui sont effectués au deuxième tour&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Pour cela, parmi les outils mathématiques de l'apprentissage automatique, nous allons utiliser des probabilités. Cette théorie va nous permettre d'exprimer le fait que les résultats tels qu'ils sont obtenus peuvent présenter une variabilité mais que celle-ci réelle résulte de préférences de chaque individu dans la population votante. En particulier, on peut considérer que chaque individu va avoir une préférence, graduée entre $0=0\%$ et $1=100\%$ pour chacun des choix (candidats, nul, blanc, abstention) au premier et second tour. Ainsi, les votes effectués vont correspondre à la réalisation de ces préférences.&lt;/p&gt;
&lt;p&gt;Bien sûr, le vote reste secret et on n'a pas accès au vote de chaque individu et encore moins à ses préférences. Mais comme chaque bureau de vote présente des variabilités liées au contexte local et qui fait que cette population locale a une préférence pour certains choix plutôt que d'autres, on peut considérer chaque bureau de vote comme une population individuelle pour lequel nous allons essayer de prédire les résultats du vote au deuxième tour. &lt;strong&gt;En exploitant les irrégularités locales, bureau de vote par bureau de vote, nous allons pouvoir prédire (le mieux possible) le report des votes individuel (de chaque individu tel qu'il passe d'un vote à un autre, par exemple de "Hidalgo" à "Macron").&lt;/strong&gt; Nous allons en particulier montrer une prédiction très correcte des données du second tour à partir de ceux du premier, montrant la plausibilité d'une telle hypothèse :&lt;/p&gt;
&lt;p&gt;&lt;img alt="Prédiction du transfert des voix" src="https://raw.githubusercontent.com/laurentperrinet/2022-05-04_transfert-des-voix/main/2022-06-08_prediction_transfert-des-voix_bce.png" title="Prédiction du transfert des voix"&gt;&lt;/p&gt;
&lt;p&gt;C'est à ma connaissance une contribution originale (jusqu'à ce qu'une bonne âme veuille bien me donner un lien vers une méthode existante similaire qui me permette de correctement la citer...) que nous allons exploiter ici. Cette prédiction, si elle est efficace (et on va montrer qu'elle est en moyenne correctement prédite avec moins de 2 points de pourcentages d'erreur près), peut donner une idée du transfert de vote entre les deux tours qui a lieu en fonction des préférences des votes de chaque individu.&lt;/p&gt;
&lt;p&gt;Nous allons dans la suite montrer comment on peut estimer le pourcentage de chances d'exprimer une voix pour un candidat ou pour l'autre en fonction du choix qu'on a exprimé au premier tour:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Transfert des voix" src="https://raw.githubusercontent.com/laurentperrinet/2022-05-04_transfert-des-voix/main/2022-06-06_transfert-des-voix.png" title="Transfert des voix"&gt;&lt;/p&gt;
&lt;p&gt;Comme on le verra plus bas, ce tableau montre des tendances claires, par exemple que si on a voté "Macron", "Jadot", "Hidalgo" ou "Pécresse" au premier tour, alors on va certainement voter "Macron" au deuxième tour. Ces électeurs se montrent particulièrement consensuel et suivent le « pacte républicain » mise en place pour faire un "barrage" au Front National (en suivant le terme consacré). Il montre aussi que si on a voté "Le Pen" ou "Dupont-Aignan" au premier tour alors on va voter Le Pen au deuxième, un clair vote de suivi.&lt;/p&gt;
&lt;p&gt;Connaissant les couleurs politiques d'autres candidats du premier tour, on peut être surpris que les électeurs de "Arthaud", "Roussel", "Lassalle" ou "Poutou" ont majoritairement choisi "Le Pen" au deuxième tour, signifiant alors un rejet du candidat Macron. Les électeurs de Zemmour sont aussi partagés, signifiant un rejet des deux alternatives. &lt;strong&gt;Ce résultat est à prendre avec des pincettes car ces derniers candidats ont obtenu moins de votes et donc que le processus d'inférence est forcément moins précis car il y a moins de données disponibles.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;En résumé, cette analyse donne des tendances en fonction des choix exprimés au premier tour:
&lt;img alt="Transfert des voix" src="https://raw.githubusercontent.com/laurentperrinet/2022-05-04_transfert-des-voix/main/2022-06-08_transfert-des-voix_tendances.png" title="Tendances présidentielle 2022"&gt;
qui montre une nette séparation des groupes de vote.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2022-06-08-transfert-de-voix.html"&gt;Read more…&lt;/a&gt; (47 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>deep-learning</category><category>grand-public</category><category>machine-learning</category><category>open-science</category><category>pandas</category><category>python</category><category>pytorch</category><guid>https://laurentperrinet.github.io/sciblog/posts/2022-06-08-transfert-de-voix.html</guid><pubDate>Wed, 08 Jun 2022 19:27:40 GMT</pubDate></item><item><title>COSYNE reviewer feedback</title><link>https://laurentperrinet.github.io/sciblog/posts/2022-02-11-cosyne-reviewer-feedback.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;tl;dr : &lt;em&gt;Crowd-sourcing raw scores for your COSYNE reviewer feedback.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Following that message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dear community,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;COSYNE is a great conference which plays a pivotal role in our field. If you have submitted an abstract (or several) you have recently received your scores. I am not affiliated to COSYNE - yet willing to contribute in some way: I would like to ask one minute of your time to report the raw scores from your reviewers. I will summarize in a few lines the results in one week time (11/02). The more numerous your feedbacks the better their precision!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As of 2022-02-20, I had received $N = 98$ answers from the &lt;a href="https://forms.gle/hjzWVemM4Jy9cBbZ9"&gt;google form&lt;/a&gt; (out of them, $95$ are valid) out of the $881$ submitted abstracts. In short, the result is that the total score $S$ is simply the linear sum of the scores $s_i$ given by each reviewer $i$ relatively weighted by the confidence levels $\pi_i$ (as stated in the email we received from the chairs):&lt;/p&gt;
&lt;p&gt;$$
S = \frac{ \sum_i \pi_i\cdot s_i}{\sum_i \pi_i}
$$&lt;/p&gt;
&lt;p&gt;Or if you prefer
$$
S = \sum_i  \frac{\pi_i}{\sum_j \pi_j} \cdot s_i
$$&lt;/p&gt;
&lt;p&gt;We deduce from that formula that the threshold is close to $6.34$ this year:&lt;/p&gt;
&lt;p&gt;&lt;img alt="2022-02-11_COSYNE-razor" src="https://github.com/laurentperrinet/2022-02-11_COSYNE-scoresheet/raw/main/2022-02-11_COSYNE-razor.png"&gt;&lt;/p&gt;
&lt;p&gt;More details in the &lt;a href="https://github.com/laurentperrinet/2022-02-11_COSYNE-scoresheet/blob/main/2022-02-11_COSYNE-scoresheet.ipynb"&gt;notebook&lt;/a&gt; (or directly in this &lt;a href="https://laurentperrinet.github.io/sciblog/posts/2022-02-11-cosyne-reviewer-feedback.html"&gt;post&lt;/a&gt;) which can also be &lt;a href="https://github.com/laurentperrinet/2022-02-11_COSYNE-scoresheet"&gt;forked here&lt;/a&gt; and &lt;a href="https://mybinder.org/v2/gh/laurentperrinet/2022-02-11_COSYNE-scoresheet/main?labpath=2022-02-11_COSYNE-scoresheet.ipynb"&gt;interactively modified on binder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;EDIT: On 2022-02-20, I have updated the notebook to account for new answers, I have now received $N = 98$ answers (out of them, $95$ are valid), yet nothing changed qualitatively. On 2022-02-11, I had received $N = 82$ answers from the &lt;a href="https://forms.gle/hjzWVemM4Jy9cBbZ9"&gt;google form&lt;/a&gt; (out of them, $79$ are valid) and the estimated threshold wass close to $6.05$.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2022-02-11-cosyne-reviewer-feedback.html"&gt;Read more…&lt;/a&gt; (12 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>blog</category><category>open-science</category><category>pandas</category><category>python</category><guid>https://laurentperrinet.github.io/sciblog/posts/2022-02-11-cosyne-reviewer-feedback.html</guid><pubDate>Fri, 11 Feb 2022 11:35:58 GMT</pubDate></item><item><title>Dreamachine</title><link>https://laurentperrinet.github.io/sciblog/posts/2022-01-30-dreamachine.html</link><dc:creator>Laurent Perrinet</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;It's at the &lt;a href="https://miam.org/"&gt;MIAM&lt;/a&gt; (Miam Musée International des Arts Modestes) in Sète, France, that I could for the first time experience really the &lt;a href="https://en.wikipedia.org/wiki/Dreamachine"&gt;Dreamachine&lt;/a&gt;. It's an optical system which consists of a central light which is periodically occluded by a rotating (cardboard?) cylinder.&lt;/p&gt;
&lt;p&gt;The magic of it is that the frequency of occlusion is around $12$ Hz, an important resonant state for sensory system. For the first time, I could really try it out at the MIAM - the important point being to close your lids and rest quiet while looking at the stroboscopic light source. Surprisingly, you see the emergence of "psychedelic patterns" (of course, less than in hippie's movies) yet of the order of the color pattern that may arise in &lt;a href="https://en.wikipedia.org/wiki/Fechner_color"&gt;Benham's Disk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It's difficult to reproduce this pattern on a screen, yet it is still possible to give an &lt;em&gt;impression of it&lt;/em&gt;. The goal is here :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;to generate a complex visual stimulation flickering on average at $12$ Hz&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;to project it on a retinotopic space to maximise the "psychedelic" effect&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="retino_alpha" src="https://laurentperrinet.github.io/sciblog/files/2022-01-30-dreamachine/retino_alpha.gif"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://laurentperrinet.github.io/sciblog/posts/2022-01-30-dreamachine.html"&gt;Read more…&lt;/a&gt; (16 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>art</category><category>blog</category><category>grand-public</category><category>motionclouds</category><category>outreach</category><category>python</category><category>space</category><category>v1</category><guid>https://laurentperrinet.github.io/sciblog/posts/2022-01-30-dreamachine.html</guid><pubDate>Sun, 30 Jan 2022 10:33:46 GMT</pubDate></item></channel></rss>